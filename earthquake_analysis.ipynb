{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b2cb772",
   "metadata": {},
   "source": [
    "Title Page / Header Cell \n",
    "\n",
    "Module: CT7201 Python Notebooks and Scripting \n",
    "\n",
    "Assignment Title: Earthquake Data Analysis Using Python Scripting (2023 Global Dataset) \n",
    "\n",
    "Student Names & IDs \n",
    "\n",
    "Date of submission \n",
    "\n",
    "Tutor Name \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6e1c61-2c7b-4cd9-8a85-58af07cf4d2f",
   "metadata": {},
   "source": [
    "Executive Summary (Short Overview) \n",
    "\n",
    "A single short paragraph that explains: \n",
    "\n",
    "What the project does \n",
    "\n",
    "What the dataset contains \n",
    "\n",
    "What analyses and models you will build \n",
    "\n",
    "The purpose of your Python scripting/OOP \n",
    "\n",
    "The key findings (a preview) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1498232a-fd5b-469e-b2a0-c949086b0959",
   "metadata": {},
   "source": [
    "Introduction \n",
    "\n",
    "Explain: \n",
    "\n",
    "Why we analyse earthquakes \n",
    "\n",
    "Why Python scripting is appropriate \n",
    "\n",
    "The importance of visualisation, functions, and clean coding \n",
    "\n",
    "A short explanation of what will be done in the notebook "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bbf123-c1ad-4a90-a704-0edd3fb9a6eb",
   "metadata": {},
   "source": [
    "Dataset Description \n",
    "\n",
    "Cover: \n",
    "\n",
    "Source: USGS Earthquake Hazards Program \n",
    "\n",
    "Scope: Global events in 2023 \n",
    "\n",
    "Number of records and variables \n",
    "\n",
    "Key fields (time, magnitude, depth, location, errors, network) \n",
    "\n",
    "Why this dataset is suitable for scripting and analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3705ea86-1889-4cff-95e8-b0a4ee5e4f5e",
   "metadata": {},
   "source": [
    "Project Objectives \n",
    "\n",
    "Write them cleanly and academically: \n",
    "\n",
    "Load, clean, and prepare the earthquake dataset using Python scripting. \n",
    "\n",
    "Implement functions and modular code to automate analysis steps. \n",
    "\n",
    "Perform univariate, bivariate, and multivariate analysis. \n",
    "\n",
    "Produce clear and readable visualisations using matplotlib/seaborn. \n",
    "\n",
    "Implement a 3D visualisation using Python libraries. \n",
    "\n",
    "Build a simple machine learning model (classification or clustering). \n",
    "\n",
    "Demonstrate good programming practice, clarity, modularity, and documentation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb73d11-1d07-404e-947b-eeae486185ad",
   "metadata": {},
   "source": [
    " Methodology \n",
    "\n",
    "A clear step-by-step description of the workflow: \n",
    "\n",
    "Import libraries \n",
    "\n",
    "Load raw CSV \n",
    "\n",
    "Clean data and handle missing values \n",
    "\n",
    "Engineer additional features \n",
    "\n",
    "Perform exploratory analysis (EDA) \n",
    "\n",
    "Build visualisations \n",
    "\n",
    "Train and evaluate a simple ML model \n",
    "\n",
    "Interpret outputs \n",
    "\n",
    "Conclude findings "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6820fa2a-c2c1-4745-8a88-bd34804fb48d",
   "metadata": {},
   "source": [
    "Python Scripting & Functions Section \n",
    "\n",
    "CT7201 marks heavily focus on scripting. \n",
    "\n",
    "You MUST: \n",
    "\n",
    "✔ Create multiple custom Python functions: \n",
    "\n",
    "load_data() \n",
    "\n",
    "clean_data() \n",
    "\n",
    "engineer_features() \n",
    "\n",
    "plot_magnitude_distribution() \n",
    "\n",
    "plot_depth_boxplot() \n",
    "\n",
    "calculate_correlations() \n",
    "\n",
    "build_classifier() \n",
    "\n",
    "plot_3D_scatter() \n",
    "\n",
    "✔ Use: \n",
    "\n",
    "docstrings \n",
    "\n",
    "comments \n",
    "\n",
    "parameters \n",
    "\n",
    "return values \n",
    "\n",
    "✔ Avoid: \n",
    "\n",
    "long messy code cells \n",
    "\n",
    "repeating the same code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cbacf1-1793-400a-93be-98bf8f04934a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T15:22:15.547172Z",
     "iopub.status.busy": "2025-12-03T15:22:15.547172Z",
     "iopub.status.idle": "2025-12-03T15:22:24.473090Z",
     "shell.execute_reply": "2025-12-03T15:22:24.473090Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import types\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Optional plotting and viz libraries\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.image as mpimg\n",
    "    from matplotlib.lines import Line2D\n",
    "    HAS_MATPLOTLIB = True\n",
    "except ImportError:\n",
    "    plt = None\n",
    "    mpimg = None\n",
    "    Line2D = None\n",
    "    HAS_MATPLOTLIB = False\n",
    "\n",
    "try:\n",
    "    import seaborn as sns\n",
    "    HAS_SEABORN = True\n",
    "except ImportError:\n",
    "    sns = None\n",
    "    HAS_SEABORN = False\n",
    "\n",
    "try:\n",
    "    import plotly.express as px\n",
    "    import plotly.graph_objects as go\n",
    "    HAS_PLOTLY = True\n",
    "except ImportError:\n",
    "    px = None\n",
    "    go = None\n",
    "    HAS_PLOTLY = False\n",
    "\n",
    "# Optional stats/ML helpers\n",
    "try:\n",
    "    from scipy.stats import gaussian_kde\n",
    "    HAS_SCIPY = True\n",
    "except ImportError:\n",
    "    gaussian_kde = None\n",
    "    HAS_SCIPY = False\n",
    "\n",
    "try:\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "    from sklearn.compose import ColumnTransformer\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "    from sklearn.cluster import KMeans, DBSCAN\n",
    "    from sklearn.decomposition import PCA\n",
    "    from sklearn.metrics import (\n",
    "        accuracy_score,\n",
    "        precision_score,\n",
    "        recall_score,\n",
    "        f1_score,\n",
    "        confusion_matrix,\n",
    "        classification_report,\n",
    "        roc_auc_score,\n",
    "        roc_curve,\n",
    "    )\n",
    "    HAS_SKLEARN = True\n",
    "except ImportError:\n",
    "    train_test_split = OneHotEncoder = StandardScaler = ColumnTransformer = Pipeline = None\n",
    "    LogisticRegression = DecisionTreeClassifier = RandomForestClassifier = GradientBoostingClassifier = None\n",
    "    KMeans = DBSCAN = PCA = None\n",
    "    accuracy_score = precision_score = recall_score = f1_score = confusion_matrix = classification_report = roc_auc_score = roc_curve = None\n",
    "    HAS_SKLEARN = False\n",
    "\n",
    "\n",
    "def apply_default_plot_style():\n",
    "    if HAS_SEABORN:\n",
    "        sns.set(style=\"whitegrid\", context=\"notebook\")\n",
    "\n",
    "\n",
    "def silence_warnings():\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "def availability():\n",
    "    return {\n",
    "        \"HAS_MATPLOTLIB\": HAS_MATPLOTLIB,\n",
    "        \"HAS_SEABORN\": HAS_SEABORN,\n",
    "        \"HAS_PLOTLY\": HAS_PLOTLY,\n",
    "        \"HAS_SKLEARN\": HAS_SKLEARN,\n",
    "        \"HAS_SCIPY\": HAS_SCIPY,\n",
    "    }\n",
    "\n",
    "\n",
    "class _Libs:\n",
    "    pass\n",
    "\n",
    "libs = _Libs()\n",
    "for name, value in {\n",
    "    \"np\": np, \"pd\": pd, \"plt\": plt, \"sns\": sns, \"mpimg\": mpimg, \"Line2D\": Line2D,\n",
    "    \"FancyBboxPatch\": None, \"FancyArrowPatch\": None,\n",
    "    \"px\": px, \"go\": go,\n",
    "    \"train_test_split\": train_test_split, \"OneHotEncoder\": OneHotEncoder, \"StandardScaler\": StandardScaler,\n",
    "    \"ColumnTransformer\": ColumnTransformer, \"Pipeline\": Pipeline,\n",
    "    \"LogisticRegression\": LogisticRegression, \"DecisionTreeClassifier\": DecisionTreeClassifier,\n",
    "    \"RandomForestClassifier\": RandomForestClassifier, \"GradientBoostingClassifier\": GradientBoostingClassifier,\n",
    "    \"KMeans\": KMeans, \"DBSCAN\": DBSCAN, \"PCA\": PCA,\n",
    "    \"accuracy_score\": accuracy_score, \"precision_score\": precision_score, \"recall_score\": recall_score,\n",
    "    \"f1_score\": f1_score, \"confusion_matrix\": confusion_matrix, \"classification_report\": classification_report,\n",
    "    \"roc_auc_score\": roc_auc_score, \"roc_curve\": roc_curve,\n",
    "    \"gaussian_kde\": gaussian_kde,\n",
    "    \"warnings\": warnings, \"os\": os, \"sys\": sys, \"json\": json, \"datetime\": datetime, \"timedelta\": timedelta,\n",
    "    \"apply_default_plot_style\": apply_default_plot_style, \"silence_warnings\": silence_warnings, \"availability\": availability,\n",
    "    \"HAS_MATPLOTLIB\": HAS_MATPLOTLIB, \"HAS_SEABORN\": HAS_SEABORN, \"HAS_PLOTLY\": HAS_PLOTLY, \"HAS_SKLEARN\": HAS_SKLEARN, \"HAS_SCIPY\": HAS_SCIPY,\n",
    "}.items():\n",
    "    setattr(libs, name, value)\n",
    "\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "if not (PROJECT_ROOT / \"data\").exists() and (PROJECT_ROOT.parent / \"data\").exists():\n",
    "    PROJECT_ROOT = PROJECT_ROOT.parent\n",
    "\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "OUTPUTS_DIR = PROJECT_ROOT / \"outputs\"\n",
    "\n",
    "try:\n",
    "    apply_default_plot_style()\n",
    "except Exception:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51694268-d60f-464b-834f-77194624f826",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T15:22:24.475512Z",
     "iopub.status.busy": "2025-12-03T15:22:24.475512Z",
     "iopub.status.idle": "2025-12-03T15:22:24.595738Z",
     "shell.execute_reply": "2025-12-03T15:22:24.594735Z"
    }
   },
   "outputs": [],
   "source": [
    "# load the raw csv file\n",
    "#Dan\n",
    "\n",
    "eq_df = pd.read_csv(DATA_DIR / \"earthquake_dataset.csv\")\n",
    "eq_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12df592-e80c-4410-b6b2-e9dace649f81",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T15:22:24.597199Z",
     "iopub.status.busy": "2025-12-03T15:22:24.597199Z",
     "iopub.status.idle": "2025-12-03T15:22:24.629561Z",
     "shell.execute_reply": "2025-12-03T15:22:24.628559Z"
    }
   },
   "outputs": [],
   "source": [
    "eq_df.describe()\n",
    "#Dan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c96824-3365-4629-8a54-5e14b35325de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T15:22:24.631593Z",
     "iopub.status.busy": "2025-12-03T15:22:24.630565Z",
     "iopub.status.idle": "2025-12-03T15:22:24.646233Z",
     "shell.execute_reply": "2025-12-03T15:22:24.645687Z"
    }
   },
   "outputs": [],
   "source": [
    "eq_df.info()\n",
    "#Dan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364c2df2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T15:22:24.648561Z",
     "iopub.status.busy": "2025-12-03T15:22:24.648561Z",
     "iopub.status.idle": "2025-12-03T15:22:24.653393Z",
     "shell.execute_reply": "2025-12-03T15:22:24.653393Z"
    }
   },
   "outputs": [],
   "source": [
    "# check maximum depth value\n",
    "eq_df[\"depth\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e83192-f8e7-4ce5-86e6-31165e9dc0b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T15:22:24.655395Z",
     "iopub.status.busy": "2025-12-03T15:22:24.655395Z",
     "iopub.status.idle": "2025-12-03T15:22:24.838440Z",
     "shell.execute_reply": "2025-12-03T15:22:24.837934Z"
    }
   },
   "outputs": [],
   "source": [
    "#use histogram to visualize the depth\n",
    "libs.plt.figure(figsize=(8,5))\n",
    "\n",
    "libs.plt.hist(eq_df['depth'], bins=20, color='red')\n",
    "libs.plt.xlim(0,700)\n",
    "libs.plt.title('Visualizing the depth of Earthquakes in dataset')\n",
    "libs.plt.xlabel('Depth(km)')\n",
    "libs.plt.ylabel('Frequency')\n",
    "libs.plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2debe3b9-e1ce-4667-b6b5-5e1bdfa07e22",
   "metadata": {},
   "source": [
    "Shows all Earthquakes falling under the maximum of 700km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e9c4fc-970b-4fa8-af4f-774a7b731210",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T15:22:24.840481Z",
     "iopub.status.busy": "2025-12-03T15:22:24.840481Z",
     "iopub.status.idle": "2025-12-03T15:22:25.079617Z",
     "shell.execute_reply": "2025-12-03T15:22:25.079617Z"
    }
   },
   "outputs": [],
   "source": [
    "#Plotting a distribution of Earthquake epicentres\n",
    "libs.plt.figure(figsize=(10, 6))\n",
    "libs.plt.scatter(eq_df['longitude'], eq_df['latitude'], s=1, alpha=0.5, c='darkblue')\n",
    "\n",
    "libs.plt.xlim(-180, 180)\n",
    "libs.plt.ylim(-90, 90)\n",
    "\n",
    "libs.plt.title('Global Distribution of Earthquake Epicenters')\n",
    "libs.plt.xlabel('Longitude (Degrees)')\n",
    "libs.plt.ylabel('Latitude (Degrees)')\n",
    "libs.plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# Optional: set to True if you want to save the PNG beside the notebook\n",
    "SAVE_SCATTER = False\n",
    "if SAVE_SCATTER and OUTPUTS_DIR is not None:\n",
    "    OUTPUTS_DIR.mkdir(exist_ok=True)\n",
    "    libs.plt.savefig(OUTPUTS_DIR / \"epicentre_scatter.png\")\n",
    "\n",
    "libs.plt.show()\n",
    "libs.plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edaf704c",
   "metadata": {},
   "source": [
    "Plot renders in the cell above when executed. Enable `SAVE_SCATTER` to also write a PNG to disk if desired.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daafcae1-1a28-4a05-9cad-120023bb8b2f",
   "metadata": {},
   "source": [
    "Visualisation to show all resords falling between the longitude and latitude ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2efc49-c3b8-4bff-840f-28beae31c90e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T15:22:25.082088Z",
     "iopub.status.busy": "2025-12-03T15:22:25.082088Z",
     "iopub.status.idle": "2025-12-03T15:22:25.086820Z",
     "shell.execute_reply": "2025-12-03T15:22:25.086820Z"
    }
   },
   "outputs": [],
   "source": [
    "#finding the distribution of type of event\n",
    "type_counts = eq_df['type'].value_counts()\n",
    "print(type_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c803fe-3f64-41c7-b01e-6a18d859aa8f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T15:22:25.088872Z",
     "iopub.status.busy": "2025-12-03T15:22:25.088872Z",
     "iopub.status.idle": "2025-12-03T15:22:25.230820Z",
     "shell.execute_reply": "2025-12-03T15:22:25.230820Z"
    }
   },
   "outputs": [],
   "source": [
    "libs.plt.figure(figsize=(8,6))\n",
    "ax = type_counts.plot(kind='bar', color = 'blue')\n",
    "libs.plt.title('Distribution of Event types within dataset')\n",
    "libs.plt.xlabel('Distribution of event types')\n",
    "libs.plt.ylabel('Frequency')\n",
    "libs.plt.xticks(rotation=45)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af453e08-610d-426b-b9d5-21c3df366899",
   "metadata": {},
   "source": [
    "The dataset predominantly holds data for earthquakes globally.  However, some of the held data refers to other disasters such as mining explosions and ice quakes.  In order to focus this analysis on earthquake prediction, the other disasters will need to be filtered out as a part of the cleaning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6f15ce-c7a2-4538-98fa-96a864f7edbe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T15:22:25.233233Z",
     "iopub.status.busy": "2025-12-03T15:22:25.233233Z",
     "iopub.status.idle": "2025-12-03T15:22:25.236362Z",
     "shell.execute_reply": "2025-12-03T15:22:25.236362Z"
    }
   },
   "outputs": [],
   "source": [
    "# show individual functions in these blocks\n",
    "#load_data():\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7649c00",
   "metadata": {},
   "source": [
    "## 3.0 Data Cleaning\n",
    "\n",
    "Reliable analysis begins with a dataset that is complete, consistent and physically meaningful. Earthquake data is particularly sensitive to measurement quality. Small errors in magnitude, depth, time or location can distort global patterns and weaken any statistical conclusions. For this reason, a structured and domain-appropriate cleaning pipeline was applied to the 2023 global earthquake dataset to ensure that every event used in this project is valid, accurate and suitable for analysis.\n",
    "\n",
    "The dataset provided by the USGS is generally well structured, but still contains a number of issues that must be addressed before performing meaningful analysis. These include occasional fully duplicated rows, multiple versions of the same earthquake (with earlier records containing outdated measurements), a small number of values that fall outside physically realistic ranges and non earthquieke type explotions. The aim of the cleaning process is not to remove large portions of the data or introduce unnecessary filtering, but simply to correct errors and ensure that each earthquake record is scientifically reasonable.\n",
    "\n",
    "The cleaning pipeline used in this project consists of the following key steps:\n",
    "\n",
    "**1. Work on a copy of the data**  \n",
    "A fresh copy of the dataset is created to protect the original raw data. This avoids accidental modifications and preserves the integrity of the input file.\n",
    "\n",
    "**2. Remove exact duplicate records**  \n",
    "Fully duplicated rows occasionally appear in exported or merged datasets. These provide no new information and would artificially inflate earthquake counts if left in place. All exact duplicates were removed.\n",
    "\n",
    "**3. Remove duplicate earthquake identifiers, keeping only the most recent update**  \n",
    "The `id` field uniquely identifies each earthquake. In some cases, the same event appears more than once because the USGS updates its magnitude, depth or location as new sensor data arrives. The dataset was sorted by the `updated` timestamp, ensuring that only the most recent and most accurate version of each event is retained.\n",
    "\n",
    "It is important to note that repeated `id` values do not represent different “stages” of an earthquake. Earthquakes do not physically change in magnitude or depth over time. Multiple entries simply reflect measurement corrections by the seismic network. Therefore, retaining only the latest updated version ensures analytical accuracy.\n",
    "\n",
    "**4. Convert time fields into proper datetime format**  \n",
    "The `time` and `updated` columns were converted from text into real datetime objects. This step is required for all time-based analysis such as monthly grouping, temporal visualisation and event ordering.\n",
    "\n",
    "**5. Ensure core numeric fields are numeric and remove missing essential values**\n",
    "The core physical attributes of each earthquake latitude, longitude, depth and magnitude must be stored as numbers.These columns were converted to numeric values using errors=\"coerce\", which safely transforms invalid entries (e.g., text or corrupted values) into NaN. Rows missing any of these essential fields were removed, as they cannot be displayed on maps or used in magnitude/depth analysis. The 2023 dataset is generally complete, so this step removed only a very small number of invalid rows.\n",
    "\n",
    "**6. Remove rows missing essential physical attributes**  \n",
    "Every earthquake must have a valid `time`, `latitude`, `longitude`, `depth` and `mag`. \n",
    "Without these, the event cannot be plotted, positioned geographically, or included in magnitude or depth analysis. Any rows missing these core fields were removed. The 2023 dataset is highly complete, so no events were lost at this stage.\n",
    "\n",
    "**7. Apply geophysical validity checks**  \n",
    "Basic physical constraints were enforced to ensure that all retained records represent real, possible earthquakes. These checks include:\n",
    "- Latitude within −90 to 90 degrees  \n",
    "- Longitude within −180 to 180 degrees  \n",
    "- The deepest earthquake in the dataset occurred at 681 km, which is within the scientifically known maximum depth for natural earthquakes (∼700 km). For this reason, depth values were restricted to the range 0–700 km to remove impossible values while keeping all real deep-focus events\n",
    "- Magnitude within the realistic range of 0 to 10\n",
    "Any record falling outside these limits was removed, as such values reflect data errors rather than genuine seismic activity.\n",
    "\n",
    "**8. Keep only real earthquakes (remove explosions and other event types)**\n",
    "The dataset includes different types of seismic events, such as quarry blasts, mining explosions, ice quakes and testing activity.These are not natural earthquakes and would distort global patterns.Only rows where type == \"earthquake\" were retained.\n",
    "\n",
    "Non-essential fields such as `gap`, `dmin`, `nst`, `horizontalError`, `magError` and \n",
    "`place` contain some missing values, but these do not prevent spatial or statistical analysis. Removing rows based on these optional metadata fields would unnecessarily reduce the dataset and bias the analysis toward only well-recorded earthquakes\n",
    "These fields were therefore kept as they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2be264-cb7d-4b50-8e23-439c16bf8af3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T15:22:25.238371Z",
     "iopub.status.busy": "2025-12-03T15:22:25.238371Z",
     "iopub.status.idle": "2025-12-03T15:22:25.244058Z",
     "shell.execute_reply": "2025-12-03T15:22:25.244058Z"
    }
   },
   "outputs": [],
   "source": [
    "#Hasini\n",
    "def clean_data(df):\n",
    "    df = df.copy() # work on a copy to avoid modifying the original dataframe\n",
    "\n",
    "    # Remove rows which are completely duplicated\n",
    "    original_rows = len(df)\n",
    "    df = df.drop_duplicates()\n",
    "    print(f\"Removed {original_rows - len(df)} exact duplicate rows.\")\n",
    "\n",
    "     # Convert 'time' and 'updated' columns to datetime\n",
    "    df[\"time\"] = pd.to_datetime(df[\"time\"], errors=\"coerce\")\n",
    "    df[\"updated\"] = pd.to_datetime(df[\"updated\"], errors=\"coerce\")\n",
    "\n",
    "    # If the same earthquake ID appears more than once, keep the most recently added entry)\n",
    "    rows_before_id_clean = len(df)\n",
    "    df = (\n",
    "        df.sort_values(\"updated\") # oldest to newest\n",
    "        .drop_duplicates(subset=\"id\", keep=\"last\")\n",
    "    )\n",
    "    print(f\"Removed {rows_before_id_clean - len(df)} rows with duplicate earthquake based on IDs.\")\n",
    "\n",
    "    # Make sure the core numerical columns are actually numbers\n",
    "    for col in [\"latitude\", \"longitude\", \"depth\", \"mag\"]:\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "    # Drop rows missing essential earthquake information\n",
    "    essential_cols = [\"time\", \"latitude\", \"longitude\", \"depth\", \"mag\"]\n",
    "    rows_before_essential_clean = len(df)\n",
    "    df = df.dropna(subset=essential_cols)\n",
    "    print(f\"Removed {rows_before_essential_clean - len(df)} rows with missing essential earthquake information in {essential_cols}.\")\n",
    "\n",
    "    # Keep only valid geographical ranges\n",
    "    rows_before_ranges = len(df)\n",
    "\n",
    "    valid_lat = df[\"latitude\"].between(-90, 90)\n",
    "    valid_lon = df[\"longitude\"].between(-180, 180)\n",
    "    valid_depth = df[\"depth\"].between(0, 700)\n",
    "    valid_mag = df[\"mag\"].between(0, 10)\n",
    "\n",
    "    df = df[valid_lat & valid_lon & valid_depth & valid_mag]\n",
    "    print(f\"Removed {rows_before_ranges - len(df)} rows with invalid geographical ranges.\")\n",
    "\n",
    "    #keep only type 'earthquake'\n",
    "    if 'type' in df.columns:\n",
    "        rows_before_type_clean = len(df)\n",
    "        df = df[df[\"type\"] == \"earthquake\"]\n",
    "        print(f\"Removed {rows_before_type_clean - len(df)} non earthquake events.\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e86ef7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T15:22:25.246063Z",
     "iopub.status.busy": "2025-12-03T15:22:25.246063Z",
     "iopub.status.idle": "2025-12-03T15:22:25.369500Z",
     "shell.execute_reply": "2025-12-03T15:22:25.369500Z"
    }
   },
   "outputs": [],
   "source": [
    "#Clean the dataset and check how its shape and structure change after cleaning.\n",
    "#Hasini\n",
    "print(\"Raw dataset shape:\", eq_df.shape)\n",
    "\n",
    "cleaned_eq_df = clean_data(eq_df)\n",
    "\n",
    "print(\"Cleaned dataset shape:\", cleaned_eq_df.shape)\n",
    "cleaned_eq_df.info()\n",
    "cleaned_eq_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8ad0f2",
   "metadata": {},
   "source": [
    "Overall, The cleaning steps removed duplicates, outdated entries, invalid coordinates, impossible depths, magnitudes and non-earthquake events—while preserving all scientifically valid measurements. No rows were removed due to missing essential fields, confirming that the dataset is generally complete.\n",
    "\n",
    "The final cleaned dataset is accurate, complete, and suitable for the exploratory analysis and machine-learning model presented later in this project. Importantly, the global structure of the data has been preserved, which is essential for analysing worldwide earthquake behaviour in 2023."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ebcb6a",
   "metadata": {},
   "source": [
    "## 4.0 Feature Engineering\n",
    "\n",
    "The cleaned 2023 USGS earthquake dataset (`cleaned_eq_df`) contains a physically consistent set of global earthquake events: valid latitude/longitude, non-negative depth, and realistic magnitude. The epicentre map from Section 3 showed events clustered along major tectonic plate boundaries, indicating the cleaning pipeline removed spurious measurements.\n",
    "\n",
    "Clean data is not automatically analytically useful. Several fields are too raw for interpretation or modelling:\n",
    "- `time` is a precise timestamp, not calendar components.\n",
    "- Depth is in km, while seismologists reason in shallow/intermediate/deep.\n",
    "- Magnitude is continuous; communication uses descriptors (moderate, strong, major).\n",
    "- Latitude/longitude are exact but don’t directly compare regions (e.g., Americas vs Asia–Pacific).\n",
    "- Data quality is spread across uncertainty fields (`gap`, `rms`, `depthError`, `magError`, `horizontalError`), not directly comparable.\n",
    "\n",
    "Implementation guardrails:\n",
    "- Leakage guard: If you later predict magnitude or depth, drop `mag_category`, `depth_category`, and `energy_log10_J` from training features to avoid label leakage.\n",
    "- Missing-data policy: Timestamps are coerced to UTC; failed parses become `NaT`. Uncertainty fields may remain `NaN`; downstream models should impute or filter using `quality_score`.\n",
    "\n",
    "We perform feature engineering: systematic transformations of raw measurements into higher-level variables useful to humans and algorithms. Engineered features are grouped into four concepts:\n",
    "1) Temporal — when earthquakes occur  \n",
    "2) Physical severity — how strong or deep they are  \n",
    "3) Geographical context — where they occur in broad global terms  \n",
    "4) Data quality — how reliable the measurements are  \n",
    "\n",
    "All transformations live in `engineer_features()`. It takes `cleaned_eq_df` and returns `featured_eq_df` with the same rows but richer columns capturing temporal patterns, physical severity, spatial context, and measurement quality. This engineered view underpins Section 5 (exploration) and Section 6 (modelling).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7713d471",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T15:22:25.371643Z",
     "iopub.status.busy": "2025-12-03T15:22:25.371643Z",
     "iopub.status.idle": "2025-12-03T15:22:25.619548Z",
     "shell.execute_reply": "2025-12-03T15:22:25.619548Z"
    }
   },
   "outputs": [],
   "source": [
    "plt = libs.plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 3.2))\n",
    "ax.axis(\"off\")\n",
    "\n",
    "def box(x, n, text):\n",
    "    ax.text(\n",
    "        x, 0.5, f\"{n}. {text}\",\n",
    "        ha=\"center\", va=\"center\",\n",
    "        fontsize=10,\n",
    "        bbox=dict(boxstyle=\"round,pad=0.6\", fc=\"white\", ec=\"black\"),\n",
    "    )\n",
    "\n",
    "box(0.15, 1,\n",
    "    r\"$\\bf{cleaned\\_eq\\_df}$\" + \"\\n\"\n",
    "    \"• Valid coordinates\\n\"\n",
    "    \"• Realistic depth & magnitude\\n\"\n",
    "    \"• Only type = 'earthquake'\")\n",
    "box(0.50, 2,\n",
    "    r\"$\\bf{engineer\\_features}$\" + \"\\n\"\n",
    "    \"• Temporal features\\n\"\n",
    "    \"• Physical severity\\n\"\n",
    "    \"• Geographic context\\n\"\n",
    "    \"• Data-quality indicators\")\n",
    "box(0.85, 3,\n",
    "    r\"$\\bf{featured\\_eq\\_df}$\" + \"\\n\"\n",
    "    \"• Same number of rows\\n\"\n",
    "    \"• New engineered columns\\n\"\n",
    "    \"• ML-ready structure\")\n",
    "\n",
    "\n",
    "plt.title(\"Feature Engineering Workflow\", fontsize=18, pad=15)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62d9af8",
   "metadata": {},
   "source": [
    "*Figure: cleaned catalogue → single `engineer_features()` pass → enriched `featured_eq_df` (same rows, richer columns: temporal, physical, spatial, quality). Re-run `engineer_features()` on any updated catalogue to regenerate an identical feature layer for comparability across years.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cda7bdf",
   "metadata": {},
   "source": [
    "### 4.1 Design of the `engineer_features()` Function\n",
    "\n",
    "Principles:\n",
    "1) Domain interpretability — features reflect seismology concepts (depth classes, magnitude labels, tectonic regions).  \n",
    "2) Reproducibility/modularity — one function regenerates the full feature set consistently.\n",
    "\n",
    "#### 4.1.1 Temporal Features — When Earthquakes Occur\n",
    "- `year`, `month`, `month_name`\n",
    "- `day`, `day_of_week`, `day_name`\n",
    "- `hour`, `part_of_day` (night/morning/afternoon/evening)\n",
    "- `is_weekend`\n",
    "- `season`\n",
    "\n",
    "#### 4.1.2 Physical Severity Features — How Strong or Deep\n",
    "- `depth_category`: shallow (0–70 km), intermediate (70–300 km), deep (300–700 km)\n",
    "- `mag_category`: minor, light, moderate, strong, major, great, massive\n",
    "- `is_strong_quake`: mag ≥ 6.0\n",
    "- `energy_log10_J`: log energy proxy (Gutenberg–Richter)\n",
    "\n",
    "Depth bins follow the standard shallow/intermediate/deep convention. Magnitude bins mirror common communication labels. Small shifts to edges don’t materially change patterns; adjust if a modelling task needs different cut points.\n",
    "\n",
    "#### 4.1.3 Geographical Context Features — Where Earthquakes Occur\n",
    "- `abs_latitude`, `abs_longitude`\n",
    "- `distance_from_equator_km`, `distance_from_prime_meridian_km`\n",
    "- `hemisphere_NS`, `hemisphere_EW`\n",
    "- `broad_region`: Americas, Europe–Africa, Asia–Pacific, Oceania\n",
    "\n",
    "`broad_region` is a coarse tectonic grouping to reveal global patterns; a finer distance-to-plate-boundary feature is a natural future enhancement when plate geometries are available.\n",
    "\n",
    "#### 4.1.4 Data-Quality Features — How Reliable the Measurements Are\n",
    "- Boolean indicators (e.g., `has_depthError`)\n",
    "- Min–max normalization of uncertainties\n",
    "- Composite `quality_score` (1 = highest quality)\n",
    "\n",
    "`quality_score` is an unweighted mean of normalized uncertainties; if some uncertainties matter more, use a weighted average or PCA-based composite as a refinement.\n",
    "\n",
    "#### 4.1.5 Categorical Encodings\n",
    "- Convenience ordinal codes `*_code` for categorical features.\n",
    "\n",
    "Categorical codes (`*_code`) are convenience ordinals; for ML, one-hot or embed them, and scale continuous features before most models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fa69bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T15:22:25.621634Z",
     "iopub.status.busy": "2025-12-03T15:22:25.621634Z",
     "iopub.status.idle": "2025-12-03T15:22:25.634253Z",
     "shell.execute_reply": "2025-12-03T15:22:25.632802Z"
    }
   },
   "outputs": [],
   "source": [
    "np = libs.np\n",
    "pd = libs.pd\n",
    "from pandas.api.types import is_datetime64_any_dtype\n",
    "\n",
    "def engineer_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Engineer temporal, physical, geographical, and data-quality features\n",
    "    from the cleaned earthquake dataframe.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # ===== 1. TEMPORAL FEATURES =====\n",
    "    if not is_datetime64_any_dtype(df[\"time\"]):\n",
    "        df[\"time\"] = pd.to_datetime(df[\"time\"], errors=\"coerce\", utc=True)\n",
    "\n",
    "    dt = df[\"time\"].dt\n",
    "    df[\"year\"] = dt.year\n",
    "    df[\"month\"] = dt.month\n",
    "    df[\"month_name\"] = dt.month_name()\n",
    "    df[\"day\"] = dt.day\n",
    "    df[\"day_of_week\"] = dt.dayofweek\n",
    "    df[\"day_name\"] = dt.day_name()\n",
    "    df[\"hour\"] = dt.hour\n",
    "    df[\"is_weekend\"] = df[\"day_of_week\"].isin([5, 6])\n",
    "\n",
    "    def hour_to_part(h):\n",
    "        if h < 6:   return \"night\"\n",
    "        if h < 12:  return \"morning\"\n",
    "        if h < 18:  return \"afternoon\"\n",
    "        return \"evening\"\n",
    "    df[\"part_of_day\"] = df[\"hour\"].apply(hour_to_part)\n",
    "\n",
    "    def month_to_season(m):\n",
    "        if m in (12, 1, 2):  return \"winter\"\n",
    "        if m in (3, 4, 5):   return \"spring\"\n",
    "        if m in (6, 7, 8):   return \"summer\"\n",
    "        return \"autumn\"\n",
    "    df[\"season\"] = df[\"month\"].apply(month_to_season)\n",
    "\n",
    "    # ===== 2. PHYSICAL SEVERITY =====\n",
    "    df[\"depth_category\"] = pd.cut(\n",
    "        df[\"depth\"],\n",
    "        bins=[0, 70, 300, 700],\n",
    "        labels=[\"shallow\", \"intermediate\", \"deep\"],\n",
    "        right=False\n",
    "    )\n",
    "    df[\"mag_category\"] = pd.cut(\n",
    "        df[\"mag\"],\n",
    "        bins=[0, 3, 4, 5, 6, 7, 8, 10],\n",
    "        labels=[\"minor\", \"light\", \"moderate\", \"strong\", \"major\", \"great\", \"massive\"],\n",
    "        right=False\n",
    "    )\n",
    "    df[\"is_strong_quake\"] = df[\"mag\"] >= 6.0\n",
    "    df[\"energy_log10_J\"] = 1.5 * df[\"mag\"] + 4.8\n",
    "\n",
    "    # ===== 3. GEOGRAPHICAL CONTEXT =====\n",
    "    df[\"abs_latitude\"] = df[\"latitude\"].abs()\n",
    "    df[\"abs_longitude\"] = df[\"longitude\"].abs()\n",
    "    df[\"distance_from_equator_km\"] = df[\"abs_latitude\"] * 111.0\n",
    "    df[\"distance_from_prime_meridian_km\"] = (\n",
    "        df[\"abs_longitude\"] * 111.0 * np.cos(np.deg2rad(df[\"latitude\"]))\n",
    "    )\n",
    "    df[\"hemisphere_NS\"] = np.where(df[\"latitude\"] >= 0, \"north\", \"south\")\n",
    "    df[\"hemisphere_EW\"] = np.where(df[\"longitude\"] >= 0, \"east\", \"west\")\n",
    "\n",
    "    def classify_region(lon):\n",
    "        if lon < -100: return \"Americas_west\"\n",
    "        if lon < -30:  return \"Americas_east_Atlantic\"\n",
    "        if lon < 60:   return \"Europe_Africa\"\n",
    "        if lon < 150:  return \"Asia_WestPacific\"\n",
    "        return \"Pacific_Oceania\"\n",
    "    df[\"broad_region\"] = df[\"longitude\"].apply(classify_region)\n",
    "\n",
    "    # ===== 4. DATA QUALITY =====\n",
    "    for col in [\"depthError\", \"magError\", \"horizontalError\"]:\n",
    "        if col in df.columns:\n",
    "            df[f\"has_{col}\"] = df[col].notna()\n",
    "\n",
    "    norm_cols = []\n",
    "    for col in [\"gap\", \"rms\", \"depthError\", \"magError\", \"horizontalError\"]:\n",
    "        if col in df.columns:\n",
    "            mn, mx = df[col].min(), df[col].max()\n",
    "            if pd.notna(mn) and pd.notna(mx) and mn != mx:\n",
    "                norm_name = f\"{col}_norm\"\n",
    "                df[norm_name] = (df[col] - mn) / (mx - mn)\n",
    "                norm_cols.append(norm_name)\n",
    "    if norm_cols:\n",
    "        df[\"quality_score\"] = 1 - df[norm_cols].mean(axis=1)\n",
    "\n",
    "    # ===== 5. CATEGORICAL ENCODINGS =====\n",
    "    cat_cols = [\n",
    "        \"depth_category\", \"mag_category\", \"hemisphere_NS\",\n",
    "        \"hemisphere_EW\", \"broad_region\", \"part_of_day\", \"season\"\n",
    "    ]\n",
    "    for c in cat_cols:\n",
    "        if c in df.columns:\n",
    "            df[f\"{c}_code\"] = df[c].astype(\"category\").cat.codes\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c261c89b",
   "metadata": {},
   "source": [
    "### 4.2 Engineered Feature Schema Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92ec9b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T15:22:25.636960Z",
     "iopub.status.busy": "2025-12-03T15:22:25.635929Z",
     "iopub.status.idle": "2025-12-03T15:22:25.696795Z",
     "shell.execute_reply": "2025-12-03T15:22:25.696795Z"
    }
   },
   "outputs": [],
   "source": [
    "# Engineered Feature Schema Summary\n",
    "featured_eq_df = engineer_features(cleaned_eq_df)\n",
    "\n",
    "# Ensure the engineered dataset exists\n",
    "try:\n",
    "    featured_eq_df\n",
    "except NameError:\n",
    "    raise NameError(\n",
    "        \"The variable 'featured_eq_df' is not defined.\\n\"\n",
    "        \"Run the feature-engineering step first:\\n\"\n",
    "        \"    featured_eq_df = engineer_features(cleaned_eq_df)\"\n",
    "    )\n",
    "\n",
    "schema_groups = {\n",
    "    \"Temporal\": [\"year\", \"month\", \"season\", \"part_of_day\", \"is_weekend\"],\n",
    "    \"Physical\": [\"depth_category\", \"mag_category\", \"is_strong_quake\", \"energy_log10_J\"],\n",
    "    \"Spatial\": [\"hemisphere_NS\", \"hemisphere_EW\", \"broad_region\", \"distance_from_equator_km\"],\n",
    "    \"Quality\": [\"quality_score\", \"gap_norm\", \"rms_norm\", \"has_depthError\"],\n",
    "    \"ML Encodings\": [\"depth_category_code\", \"mag_category_code\", \"season_code\", \"part_of_day_code\"],\n",
    "}\n",
    "\n",
    "print(\"Engineered Feature Schema Summary\\n(showing only columns present in featured_eq_df)\\n\")\n",
    "\n",
    "total_listed = set()\n",
    "for group_name, columns in schema_groups.items():\n",
    "    existing = [col for col in columns if col in featured_eq_df.columns]\n",
    "    total_listed.update(existing)\n",
    "    print(f\"{group_name} ({len(existing)} features):\")\n",
    "    for col in existing:\n",
    "        print(f\"  • {col}\")\n",
    "    print()\n",
    "\n",
    "print(f\"Total engineered features listed: {len(total_listed)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77a0bf3",
   "metadata": {},
   "source": [
    "The schema summary above provides a structured overview of all engineered\n",
    "features added to the cleaned earthquake dataset. The features are grouped into\n",
    "five conceptual domains Temporal, Physical, Spatial, Quality, and ML Encodings \n",
    "each serving a distinct analytical purpose.\n",
    "\n",
    "The temporal group confirms that the original timestamp has been decomposed into\n",
    " interpretable components such as month, season, and part of day, enabling\n",
    "fine-grained temporal analyses without relying on raw datetime strings. The\n",
    "physical group shows the conversion of depth and magnitude into meaningful\n",
    "severity descriptors, including depth categories and magnitude classes, which\n",
    "align with standard seismological conventions. Spatial features provide\n",
    "higher-level geographical context by transforming raw latitude and longitude\n",
    "into hemispheres and broad tectonic regions. The quality indicators summarise\n",
    "uncertainty and measurement reliability across several USGS-provided metrics,\n",
    "allowing later analyses to differentiate between well-constrained and\n",
    "poorly-constrained events.\n",
    "\n",
    "The final group, ML Encodings, contains the integer representations of\n",
    "categorical variables that are required for downstream modelling. These codes\n",
    "are machine-readable placeholders and should not be interpreted as ordinal\n",
    "quantities; for modelling, they will typically be one-hot encoded or embedded.\n",
    "\n",
    "Overall, this schema demonstrates how the feature-engineering pipeline lifts the\n",
    "raw USGS catalogue into a structured, analysis-ready dataset, with each feature\n",
    "group supporting a different aspect of the exploratory and modelling workflows\n",
    "that follow.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d529c3",
   "metadata": {},
   "source": [
    "### 4.3 Feature Engineering Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8414ba2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T15:22:25.698901Z",
     "iopub.status.busy": "2025-12-03T15:22:25.698901Z",
     "iopub.status.idle": "2025-12-03T15:22:25.770637Z",
     "shell.execute_reply": "2025-12-03T15:22:25.770081Z"
    }
   },
   "outputs": [],
   "source": [
    "# Applying the Feature Engineering Pipeline\n",
    "\n",
    "print(\"Applying feature engineering to cleaned_eq_df...\\n\")\n",
    "\n",
    "# Record original column names\n",
    "original_columns = list(cleaned_eq_df.columns)\n",
    "\n",
    "# Apply the feature engineering function\n",
    "featured_eq_df = engineer_features(cleaned_eq_df)\n",
    "\n",
    "# Shape comparison\n",
    "print(\"=== Dataset Shape Comparison ===\")\n",
    "print(f\"Before feature engineering : {cleaned_eq_df.shape}\")\n",
    "print(f\"After feature engineering  : {featured_eq_df.shape}\")\n",
    "\n",
    "# Identify new engineered features\n",
    "new_cols = [c for c in featured_eq_df.columns if c not in original_columns]\n",
    "\n",
    "print(f\"\\nNumber of engineered features added: {len(new_cols)}\")\n",
    "print(\"Sample of engineered feature names (first 20):\")\n",
    "for c in sorted(new_cols)[:20]:\n",
    "    print(f\"  • {c}\")\n",
    "\n",
    "# Display the first 5 rows of the engineered dataset\n",
    "print(\"\\nPreview of engineered dataframe:\")\n",
    "featured_eq_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613714f0",
   "metadata": {},
   "source": [
    "The output above confirms that the feature engineering function has been applied\n",
    "successfully to the cleaned USGS catalogue. The number of rows remains unchanged,\n",
    "demonstrating that feature engineering enriches the dataset without altering the\n",
    "underlying set of earthquake events. This is an essential property, as it ensures\n",
    "that all subsequent analyses are based on the same physical observations that\n",
    "were validated during the cleaning stage.\n",
    "\n",
    "The increase in the number of columns reflects the addition of new engineered\n",
    "features across the temporal, physical, spatial, and data-quality groups\n",
    "introduced in Section 4.1. The summary shows how many new variables were created\n",
    "and provides a sample of their names, illustrating how the raw fields have been\n",
    "transformed into a richer and more analytically expressive representation.\n",
    "\n",
    "The preview of `featured_eq_df` reveals the final structure of the enriched\n",
    "dataset, allowing us to verify that the engineered features have been correctly\n",
    "integrated. This includes checks such as: the presence of categorical descriptors\n",
    "(e.g., `depth_category`, `mag_category`), temporal components (e.g., `month`,\n",
    "`season`), spatial context variables (e.g., `broad_region`), and normalised\n",
    "uncertainty metrics (e.g., `gap_norm`, `rms_norm`).\n",
    "\n",
    "Overall, the pipeline output demonstrates that the dataset is now fully prepared\n",
    "for exploratory analysis (Section 5) and for use as structured input in\n",
    "machine-learning workflows (Section 6). The engineered features provide a more\n",
    "interpretable, domain-aligned foundation for uncovering meaningful patterns in\n",
    "global seismic activity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e68d79f",
   "metadata": {},
   "source": [
    "### 4.4 Overview of Engineered Feature Groups\n",
    "\n",
    "To validate that the engineered features behave sensibly and to provide an\n",
    "at-a-glance summary of the enriched dataset, Figure 4.4 presents a four-panel\n",
    "overview of the main feature groups. These panels correspond to:  \n",
    "(a) temporal earthquake frequency,  \n",
    "(b) magnitude severity categories,  \n",
    "(c) spatial distribution across broad global regions, and  \n",
    "(d) distribution of the composite measurement-quality score.\n",
    "\n",
    "Together, these visualisations provide a compact diagnostic view of the\n",
    "engineered feature space, confirming that the transformations applied in Section\n",
    "4.1 produce interpretable and domain-consistent structures.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747f6e1f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T15:22:25.772807Z",
     "iopub.status.busy": "2025-12-03T15:22:25.772807Z",
     "iopub.status.idle": "2025-12-03T15:22:26.263735Z",
     "shell.execute_reply": "2025-12-03T15:22:26.263735Z"
    }
   },
   "outputs": [],
   "source": [
    "plt = libs.plt\n",
    "np = libs.np\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "axes = axes.ravel()\n",
    "labels = ['(a)', '(b)', '(c)', '(d)']\n",
    "\n",
    "# (a) Monthly frequency\n",
    "order_months = [\n",
    "    \"January\",\"February\",\"March\",\"April\",\"May\",\"June\",\n",
    "    \"July\",\"August\",\"September\",\"October\",\"November\",\"December\"\n",
    "]\n",
    "month_counts = (\n",
    "    featured_eq_df[\"month_name\"]\n",
    "    .value_counts()\n",
    "    .reindex(order_months)\n",
    "    .fillna(0)\n",
    ")\n",
    "x = np.arange(len(order_months))\n",
    "axes[0].plot(x, month_counts.values, marker=\"o\", linewidth=2)\n",
    "axes[0].set_title(\"Monthly earthquake frequency\", fontsize=12)\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(order_months, rotation=45, ha=\"right\")\n",
    "axes[0].set_ylabel(\"Number of earthquakes\")\n",
    "axes[0].grid(axis=\"y\", linestyle=\"--\", alpha=0.3)\n",
    "axes[0].text(-0.12, 1.05, labels[0], transform=axes[0].transAxes,\n",
    "             fontsize=13, fontweight=\"bold\")\n",
    "\n",
    "# (b) Magnitude categories\n",
    "mag_counts = (\n",
    "    featured_eq_df[\"mag_category\"]\n",
    "    .value_counts()\n",
    "    .sort_index()\n",
    ")\n",
    "y_pos = np.arange(len(mag_counts))\n",
    "axes[1].barh(y_pos, mag_counts.values, color=\"#cfe2ff\", edgecolor=\"#2f2f2f\")\n",
    "axes[1].set_yticks(y_pos)\n",
    "axes[1].set_yticklabels(mag_counts.index.astype(str))\n",
    "axes[1].invert_yaxis()\n",
    "axes[1].set_title(\"Magnitude category distribution\", fontsize=12)\n",
    "axes[1].set_xlabel(\"Number of earthquakes\")\n",
    "axes[1].grid(axis=\"x\", linestyle=\"--\", alpha=0.3)\n",
    "axes[1].text(-0.12, 1.05, labels[1], transform=axes[1].transAxes,\n",
    "             fontsize=13, fontweight=\"bold\")\n",
    "for i, v in enumerate(mag_counts.values):\n",
    "    axes[1].text(v + mag_counts.values.max()*0.01, i, str(v),\n",
    "                 va=\"center\", fontsize=9)\n",
    "\n",
    "# (c) Broad regions\n",
    "region_counts = (\n",
    "    featured_eq_df[\"broad_region\"]\n",
    "    .value_counts()\n",
    "    .sort_index()\n",
    ")\n",
    "x2 = np.arange(len(region_counts))\n",
    "axes[2].bar(x2, region_counts.values, color=\"#ffe5d9\", edgecolor=\"#2f2f2f\")\n",
    "axes[2].set_xticks(x2)\n",
    "axes[2].set_xticklabels(region_counts.index.astype(str), rotation=20, ha=\"right\")\n",
    "axes[2].set_title(\"Earthquake counts by broad region\", fontsize=12)\n",
    "axes[2].set_ylabel(\"Number of earthquakes\")\n",
    "axes[2].grid(axis=\"y\", linestyle=\"--\", alpha=0.3)\n",
    "axes[2].text(-0.12, 1.05, labels[2], transform=axes[2].transAxes,\n",
    "             fontsize=13, fontweight=\"bold\")\n",
    "\n",
    "# (d) Quality score\n",
    "ax = axes[3]\n",
    "if \"quality_score\" in featured_eq_df.columns:\n",
    "    qs = featured_eq_df[\"quality_score\"].dropna()\n",
    "    ax.hist(qs, bins=20, edgecolor=\"black\", alpha=0.7, color=\"#d1e7dd\")\n",
    "    median_qs = qs.median()\n",
    "    ax.axvline(median_qs, color=\"red\", linestyle=\"--\", linewidth=1.5,\n",
    "               label=f\"Median = {median_qs:.2f}\")\n",
    "    ax.set_title(\"Quality score distribution\", fontsize=12)\n",
    "    ax.set_xlabel(\"Quality score (1 = best)\")\n",
    "    ax.set_ylabel(\"Frequency\")\n",
    "    ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.3)\n",
    "    ax.legend(fontsize=9)\n",
    "else:\n",
    "    ax.text(0.5, 0.5, \"quality_score not available\",\n",
    "            ha=\"center\", va=\"center\", fontsize=11)\n",
    "    ax.axis(\"off\")\n",
    "ax.text(-0.12, 1.05, labels[3], transform=ax.transAxes,\n",
    "        fontsize=13, fontweight=\"bold\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f4d229",
   "metadata": {},
   "source": [
    "**Panel (a)** shows that global seismic activity in 2023 is distributed relatively\n",
    "evenly across the calendar year, with no extreme seasonal peaks. This aligns\n",
    "with the expectation that tectonic processes are not seasonally driven.\n",
    "\n",
    "**Panel (b)** illustrates the dominance of the lower magnitude classes\n",
    "(minor–moderate) and the rarity of high-magnitude events, consistent with the\n",
    "Gutenberg–Richter relationship. The engineered `mag_category` feature clearly\n",
    "captures this well-known frequency–magnitude structure.\n",
    "\n",
    "**Panel (c)** highlights the strong regional variations in earthquake occurrence.\n",
    "The Asia–Pacific and American subduction-zone regions show the highest counts,\n",
    "matching the distribution of major plate boundaries. This validates the use of\n",
    "the `broad_region` engineered feature.\n",
    "\n",
    "**Panel (d)** summarises the distribution of the composite `quality_score`,\n",
    "showing that most events have mid-to-high quality measurements, with a small\n",
    "tail of lower-quality observations. This provides useful diagnostic insight for\n",
    "selecting high-confidence subsets in later modelling.\n",
    "\n",
    "Together, these panels confirm that the engineered features behave logically,\n",
    "preserve known seismological patterns, and are suitable for downstream analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f9b2cb",
   "metadata": {},
   "source": [
    "### 4.5 Additional Feature-Level Visualisations\n",
    "\n",
    "To complement the high-level overview in Figure 4.4, this subsection focuses on\n",
    "individual engineered features using more expressive visual designs. The first\n",
    "plot revisits the magnitude categories and presents them as a horizontal severity\n",
    "profile, which reads more like a risk gradient than a simple frequency table.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd12f83",
   "metadata": {},
   "source": [
    "#### 4.5.1 Magnitude severity profile\n",
    "\n",
    "Figure 4.5 presents the distribution of the engineered `mag_category` variable\n",
    "using a horizontal bar layout. Magnitude classes are ordered from lowest to\n",
    "highest severity, allowing the reader to see at a glance how the global 2023\n",
    "earthquake catalogue is dominated by lower-magnitude events, with progressively\n",
    "smaller counts in the higher-severity bands.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de19ec8c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T15:22:26.265860Z",
     "iopub.status.busy": "2025-12-03T15:22:26.265860Z",
     "iopub.status.idle": "2025-12-03T15:22:26.396515Z",
     "shell.execute_reply": "2025-12-03T15:22:26.396515Z"
    }
   },
   "outputs": [],
   "source": [
    "plt = libs.plt\n",
    "np = libs.np\n",
    "\n",
    "mag_counts = (\n",
    "    featured_eq_df[\"mag_category\"]\n",
    "    .value_counts()\n",
    "    .sort_index()\n",
    ")\n",
    "\n",
    "colors = plt.cm.plasma(np.linspace(0.25, 0.95, len(mag_counts)))\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.barh(\n",
    "    mag_counts.index.astype(str),\n",
    "    mag_counts.values,\n",
    "    color=colors,\n",
    "    edgecolor=\"#2f2f2f\"\n",
    ")\n",
    "\n",
    "plt.title(\"Magnitude severity profile (enhanced view)\", fontsize=13)\n",
    "plt.xlabel(\"Number of earthquakes\")\n",
    "plt.ylabel(\"Magnitude category\")\n",
    "plt.grid(axis=\"x\", linestyle=\"--\", alpha=0.4)\n",
    "\n",
    "# Add counts at bar ends\n",
    "for i, v in enumerate(mag_counts.values):\n",
    "    plt.text(\n",
    "        v + max(mag_counts.values) * 0.01,\n",
    "        i,\n",
    "        str(v),\n",
    "        va=\"center\",\n",
    "        fontsize=9\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d41205",
   "metadata": {},
   "source": [
    "The plot reveals a clear ordered risk gradient: the bulk of events fall into the\n",
    "minor and light magnitude classes, while the strong, major and great categories\n",
    "form a much thinner tail. This pattern is consistent with the expected\n",
    "frequency–magnitude behaviour of global seismicity and confirms that the\n",
    "engineered `mag_category` feature captures the underlying severity structure of\n",
    "the dataset in a compact, interpretable form.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfc0047",
   "metadata": {},
   "source": [
    "#### 4.5.2 Magnitude distribution by depth category\n",
    "\n",
    "The next visualisation examines the relationship between earthquake depth and\n",
    "magnitude using a violin plot. Unlike a boxplot, which only displays quartiles,\n",
    "a violin plot shows the entire probability density of the data, making it easier\n",
    "to see differences in distribution shape across depth categories.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf41265",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T15:22:26.398534Z",
     "iopub.status.busy": "2025-12-03T15:22:26.398534Z",
     "iopub.status.idle": "2025-12-03T15:22:26.544940Z",
     "shell.execute_reply": "2025-12-03T15:22:26.544940Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9, 5))\n",
    "data = [featured_eq_df[featured_eq_df[\"depth_category\"] == cat][\"mag\"].dropna()\n",
    "    for cat in [\"shallow\", \"intermediate\", \"deep\"]]\n",
    "\n",
    "plt.violinplot(data, showmeans=True, showextrema=True)\n",
    "plt.title(\"Magnitude distribution by depth category\", fontsize=13)\n",
    "plt.xlabel(\"Depth category\")\n",
    "plt.ylabel(\"Magnitude\")\n",
    "plt.xticks([1, 2, 3], [\"Shallow\", \"Intermediate\", \"Deep\"])\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.4)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f4bc8d",
   "metadata": {},
   "source": [
    "The violin plot illustrates how the magnitude distribution varies across depth\n",
    "classes. Shallow earthquakes display a wider density spread, indicating a larger\n",
    "range of magnitudes and higher overall variability. Intermediate events show a\n",
    "narrower distribution, while deep earthquakes tend to cluster toward moderate\n",
    "magnitudes with fewer extreme events.\n",
    "\n",
    "The width and tails of each violin demonstrate how the likelihood of different\n",
    "magnitudes changes with depth, reflecting the physical differences between\n",
    "shallow crustal events and deeper subduction-zone processes. This confirms that\n",
    "the engineered `depth_category` and `mag_category` features interact in\n",
    "meaningful, domain-consistent ways.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f244584c",
   "metadata": {},
   "source": [
    "#### 4.5.3 Global epicentre distribution with magnitude–depth\n",
    "\n",
    "The final visualisation in this subsection brings together the engineered spatial\n",
    "and physical-severity features. Each earthquake is plotted on a global\n",
    "longitude–latitude grid, with marker colour representing magnitude and marker\n",
    "size encoding the engineered depth class. This produces a combined\n",
    "severity–depth visual signature that highlights global tectonic patterns in a\n",
    "way that raw coordinates alone cannot reveal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e2cae5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T15:22:26.547482Z",
     "iopub.status.busy": "2025-12-03T15:22:26.547482Z",
     "iopub.status.idle": "2025-12-03T15:22:27.085647Z",
     "shell.execute_reply": "2025-12-03T15:22:27.084648Z"
    }
   },
   "outputs": [],
   "source": [
    "plt = libs.plt\n",
    "mpimg = libs.mpimg\n",
    "np = libs.np\n",
    "pd = libs.pd\n",
    "Line2D = libs.Line2D\n",
    "\n",
    "\n",
    "# 1. LOAD AND CROP WORLD MAP (SOFTER, HIGHER CONTRAST)\n",
    "\n",
    "img = mpimg.imread(DATA_DIR / \"world_map.png\")\n",
    "\n",
    "# Crop black borders\n",
    "rgb = img[:, :, :3]\n",
    "brightness = rgb.mean(axis=2)\n",
    "mask = brightness > 0.02\n",
    "rows = np.where(mask.any(axis=1))[0]\n",
    "cols = np.where(mask.any(axis=0))[0]\n",
    "cropped_img = img[rows[0]:rows[-1], cols[0]:cols[-1]]\n",
    "\n",
    "# Slight contrast adjustment (not too dark)\n",
    "cropped_img = np.clip(cropped_img * 0.9, 0, 1)\n",
    "\n",
    "\n",
    "# 2. IMPORT TECTONIC PLATE BOUNDARIES\n",
    "\n",
    "plates = pd.read_csv(DATA_DIR / \"plate_boundaries.csv\")\n",
    "\n",
    "\n",
    "# 3. Marker sizes by depth category\n",
    "\n",
    "size_map = {\"shallow\": 32, \"intermediate\": 20, \"deep\": 12}\n",
    "marker_sizes = (\n",
    "    featured_eq_df[\"depth_category\"]\n",
    "    .map(size_map)\n",
    "    .astype(float)\n",
    "    .fillna(14.0)\n",
    ")\n",
    "\n",
    "\n",
    "# 4. Create final figure\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "# Background world map\n",
    "ax.imshow(\n",
    "    cropped_img,\n",
    "    extent=[-180, 180, -90, 90],\n",
    "    aspect=\"auto\",\n",
    "    alpha=0.75,      # slightly more transparent so data stands out\n",
    "    zorder=0\n",
    ")\n",
    "\n",
    "# Earthquake points\n",
    "sc = ax.scatter(\n",
    "    featured_eq_df[\"longitude\"],\n",
    "    featured_eq_df[\"latitude\"],\n",
    "    c=featured_eq_df[\"mag\"],\n",
    "    s=marker_sizes,\n",
    "    alpha=0.8,\n",
    "    cmap=\"viridis\",\n",
    "    edgecolors=\"none\",\n",
    "    zorder=2\n",
    ")\n",
    "\n",
    "\n",
    "# 5. Tectonic plate boundaries overlay\n",
    "\n",
    "ax.plot(\n",
    "    plates[\"lon\"], plates[\"lat\"],\n",
    "    color=\"red\", linewidth=1.3, linestyle=\"--\",\n",
    "    alpha=0.8, zorder=3\n",
    ")\n",
    "\n",
    "\n",
    "# 6. Hotspot annotations\n",
    "\n",
    "hotspots = {\n",
    "    \"Japan\": (140, 38),\n",
    "    \"Chile\": (-72, -30),\n",
    "    \"Turkey\": (35, 39),\n",
    "    \"Tonga\": (-175, -20),    # already in -180..180\n",
    "    \"California\": (-122, 37)\n",
    "}\n",
    "\n",
    "for name, (x, y) in hotspots.items():\n",
    "    ax.scatter(x, y, s=60, c=\"red\", edgecolors=\"white\", linewidth=0.7, zorder=4)\n",
    "    ax.text(\n",
    "        x + 4, y + 4,\n",
    "        name,\n",
    "        fontsize=9,\n",
    "        color=\"red\",\n",
    "        fontweight=\"bold\",\n",
    "        zorder=5\n",
    "    )\n",
    "\n",
    "\n",
    "# 7. Annotate key tectonic provinces (black labels)\n",
    "tectonic_labels = {\n",
    "    \"Andes\": (-70, -15),\n",
    "    \"Cascadia\": (-125, 45),\n",
    "    \"Mediterranean Belt\": (20, 38),\n",
    "    \"Himalayas\": (80, 30),\n",
    "    \"East Africa Rift\": (35, 2),\n",
    "    \"Indonesia Arc\": (120, -2),\n",
    "    \"Japan Trench\": (145, 38),\n",
    "    \"Tonga Arc\": (-175, -18),\n",
    "}\n",
    "\n",
    "for name, (lon, lat) in tectonic_labels.items():\n",
    "    ax.text(\n",
    "        lon, lat, name,\n",
    "        fontsize=9, color=\"black\",\n",
    "        ha=\"center\", va=\"center\",\n",
    "        alpha=1, zorder=4,\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 8. Axis, ticks, grids\n",
    "\n",
    "ax.set_title(\n",
    "    \"Global earthquake epicentres (2023)\\n\"\n",
    "    \"with tectonic boundaries and hotspot annotations\",\n",
    "    fontsize=20\n",
    ")\n",
    "ax.set_xlabel(\"Longitude (°)\")\n",
    "ax.set_ylabel(\"Latitude (°)\")\n",
    "\n",
    "ax.set_xlim(-180, 180)\n",
    "ax.set_ylim(-90, 90)\n",
    "ax.set_xticks(np.arange(-180, 181, 60))\n",
    "ax.set_yticks(np.arange(-90, 91, 30))\n",
    "ax.grid(True, linestyle=\"--\", alpha=0.25)\n",
    "\n",
    "\n",
    "# 9. Colourbar for magnitude\n",
    "\n",
    "cbar = plt.colorbar(sc, ax=ax)\n",
    "cbar.set_label(\"Magnitude\")\n",
    "\n",
    "\n",
    "# 10. Legends (depth + boundaries)\n",
    "\n",
    "depth_legend_elements = [\n",
    "    Line2D(\n",
    "        [0], [0],\n",
    "        marker='o', linestyle='',\n",
    "        markersize=np.sqrt(size_map[\"shallow\"]),\n",
    "        label='Shallow (0–70 km)',\n",
    "        markerfacecolor='grey', alpha=0.8\n",
    "    ),\n",
    "    Line2D(\n",
    "        [0], [0],\n",
    "        marker='o', linestyle='',\n",
    "        markersize=np.sqrt(size_map[\"intermediate\"]),\n",
    "        label='Intermediate (70–300 km)',\n",
    "        markerfacecolor='grey', alpha=0.8\n",
    "    ),\n",
    "    Line2D([0], [0], marker='o', linestyle='',\n",
    "        markersize=np.sqrt(size_map[\"deep\"]),\n",
    "        label='Deep (300–700 km)',\n",
    "        markerfacecolor='grey', alpha=0.8\n",
    "    ),\n",
    "]\n",
    "\n",
    "# First legend: depth categories\n",
    "depth_legend = ax.legend(\n",
    "    handles=depth_legend_elements,\n",
    "    title=\"Depth category\",\n",
    "    loc=\"lower left\",\n",
    "    fontsize=8,\n",
    "    title_fontsize=9,\n",
    "    frameon=True\n",
    ")\n",
    "ax.add_artist(depth_legend)\n",
    "\n",
    "# Second legend: tectonic boundaries + hotspots\n",
    "boundary_hotspot_legend = ax.legend(\n",
    "    handles=[\n",
    "        Line2D([0], [0], color=\"red\", linestyle=\"--\", linewidth=1.3,\n",
    "               label=\"Tectonic plate boundaries\"),\n",
    "        Line2D([0], [0], marker=\"o\", linestyle=\"\", markersize=6,\n",
    "               markerfacecolor=\"red\", markeredgecolor=\"white\",\n",
    "               label=\"Annotated hotspot\")\n",
    "    ],\n",
    "    loc=\"lower right\",\n",
    "    fontsize=8,\n",
    "    frameon=True\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fcf567",
   "metadata": {},
   "source": [
    "## Interpretation of Global Earthquake Epicentres\n",
    "\n",
    "The enhanced global epicentre map presents a geophysically coherent distribution of earthquake activity, with events aligning precisely along the major tectonic plate interfaces. The strongest and most persistent seismic zones—such as the **Pacific Ring of Fire**, the **Japan–Kurile arc**, the **Tonga–Kermadec subduction system**, the **Indonesian convergence zone**, and the **Peru–Chile trench**—appear as continuous, high-density belts of epicentres. These alignments coincide exactly with expected plate boundaries, confirming both the geographical accuracy of the cleaned dataset and the correctness of the engineered spatial variables.\n",
    "\n",
    "The visual encoding integrates two fundamental physical dimensions of earthquakes:  \n",
    "- **magnitude**, represented by a perceptually uniform colour scale, and  \n",
    "- **focal depth**, expressed through marker size.  \n",
    "\n",
    "This dual encoding reveals important physical structure. **Shallow earthquakes** (0–70 km), shown as larger markers, dominate global seismicity and cluster densely along subduction interfaces, where interplate friction generates frequent, high-magnitude thrust events. **Intermediate-depth earthquakes** (70–300 km) trace well-defined Wadati–Benioff zones beneath Japan, Tonga, Indonesia and the Andean margin, illustrating the geometry of subducting slabs. **Deep-focus earthquakes** (300–700 km), while comparatively rare, appear exclusively within these subduction corridors—consistent with established geodynamic models of cold, sinking lithosphere.\n",
    "\n",
    "The inclusion of annotated hotspots (Japan, Chile, Turkey, Tonga, California) further emphasises regions of chronic stress accumulation and complex fault interaction. Complementary regional labels—such as the **Mediterranean Belt**, **Himalayas**, **Cascadia**, and the **East African Rift**—demonstrate that the engineered geographic variables (e.g. `broad_region`, `hemisphere_NS`, `distance_from_equator_km`) correctly segment the dataset into meaningful tectonic provinces.\n",
    "\n",
    "Overall, the figure provides strong validation of the feature-engineering pipeline. The enriched dataset preserves the expected structural patterns of global seismicity, while the engineered attributes (depth categories, magnitude classes, distance measures, and region codes) make previously implicit geophysical relationships more observable. The map therefore functions as both a diagnostic check of dataset integrity and a substantive foundation for the exploratory and modelling work presented in subsequent sections.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97ac0e9",
   "metadata": {},
   "source": [
    "### 4.6 Summary and Reflection\n",
    "\n",
    "The feature-engineering stage transformed the cleaned earthquake catalogue into a structured, analysis-ready dataset that captures the multi-dimensional nature of global seismicity. The `engineer_features()` function integrates temporal descriptors (e.g. month, season, diurnal cycle), physical severity indicators (magnitude class, depth category, energy proxy), spatial context (hemispheres, distance measures, broad tectonic regions), and data-quality metrics (normalised uncertainty measures and composite quality scores). Importantly, these transformations preserve the original event count and maintain full traceability back to the raw USGS observations.\n",
    "\n",
    "The diagnostic visualisations confirm that the engineered features behave in a geophysically meaningful way. Temporal variables reveal realistic seasonal and monthly variations in global seismic reporting. Depth and magnitude classes highlight the dominance of shallow crustal earthquakes and the physical structure of subduction-driven Wadati–Benioff zones. Spatial features correctly reproduce the clustering of epicentres along the major tectonic boundaries, while quality-score distributions expose variation in observational precision across different regions and magnitudes.\n",
    "\n",
    "From an analytical perspective, the engineered dataset offers a substantially richer basis for downstream exploration and modelling. The introduction of interpretable categories improves descriptive analysis, while the numeric encodings (e.g. `*_code`) provide a machine-learning–friendly representation. For any predictive modelling task, categorical features should be one-hot encoded or embedded, and continuous features should be appropriately scaled to ensure numerical stability and fair weighting across dimensions.\n",
    "\n",
    "A key strength of the approach is its reproducibility: because `engineer_features()` is implemented as a modular function, it can be applied to future USGS catalogues with no manual intervention. This guarantees that new datasets will inherit the same engineered structure, ensuring consistent analysis pipelines over time. Overall, the feature-engineering process successfully bridges raw seismic measurements and structured analytical insight, forming the foundation for the exploratory analysis and modelling undertaken in the subsequent sections.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b807d8d0",
   "metadata": {},
   "source": [
    "- 5. Object-Oriented Design & Implementation  \n",
    "5.1 EarthquakeDataset Class  \n",
    "5.2 EarthquakeVisualizer Class  \n",
    "5.3 EarthquakeModel Class (optional)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220e92c5",
   "metadata": {},
   "source": [
    "- 6. Data Preparation (Using the OOP Pipeline)  \n",
    "6.1 Loading the Dataset  \n",
    "6.2 Cleaning the Dataset  \n",
    "6.3 Feature Engineering  \n",
    "6.4 Final Processed Dataset  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38173a1-31a8-497e-9969-30f35e63d794",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T15:22:27.088077Z",
     "iopub.status.busy": "2025-12-03T15:22:27.088077Z",
     "iopub.status.idle": "2025-12-03T15:22:27.093641Z",
     "shell.execute_reply": "2025-12-03T15:22:27.093641Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Library availability (inline helper):\")\n",
    "print(libs.availability())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7264685",
   "metadata": {},
   "source": [
    "- 7. Univariate Analysis (EDA – Part 1)  #DAN   \n",
    "7.1 Magnitude Distribution (KDE)  \n",
    "7.2 Depth Distribution (Histogram & Boxplot)  \n",
    "7.3 Station Count (nst / magNst)  \n",
    "7.4 Distance to Nearest Station (dmin)  \n",
    "7.5 Temporal Frequency (Monthly / Daily Counts)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cd9f36-d565-4cc2-ba23-42497322fe9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a26b62-d0e4-4ab3-8a8b-0cdaf17fd2f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T15:22:27.095674Z",
     "iopub.status.busy": "2025-12-03T15:22:27.095674Z",
     "iopub.status.idle": "2025-12-03T15:22:27.476978Z",
     "shell.execute_reply": "2025-12-03T15:22:27.476446Z"
    }
   },
   "outputs": [],
   "source": [
    "#Magnitude distribution\n",
    "conditions =[\n",
    "    (eq_df['depth'] >= 0) & (eq_df['depth'] <= 70),\n",
    "    (eq_df['depth'] > 70) & (eq_df['depth'] <= 300),\n",
    "    (eq_df['depth'] > 300) & (eq_df['depth'] <= 700)\n",
    "]\n",
    "\n",
    "depth_ranges = ['Shallow(0-70 km)', 'Intermediate (70-300 km)', 'Deep-focus (300-700 km)']\n",
    "eq_df['depth_category'] = np.select(conditions, depth_ranges, default = 'invalid')\n",
    "\n",
    "plt = libs.plt\n",
    "sns = libs.sns\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "for depth_range in depth_ranges:\n",
    "    subset=eq_df[eq_df['depth_category'] == depth_range]\n",
    "    sns.kdeplot(subset['mag'],\n",
    "                label=depth_range,\n",
    "                fill=True,\n",
    "                alpha = 0.5)\n",
    "\n",
    "plt.title('Magnitude distribution by depth category')\n",
    "plt.xlabel('Magnitude')\n",
    "plt.ylabel('Density')\n",
    "plt.xlim(0,10)\n",
    "plt.legend(title='Earthquake depth category')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d541b6-62f5-4dbb-b853-7e2a48e38864",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T15:22:27.479067Z",
     "iopub.status.busy": "2025-12-03T15:22:27.479067Z",
     "iopub.status.idle": "2025-12-03T15:22:27.759931Z",
     "shell.execute_reply": "2025-12-03T15:22:27.758917Z"
    }
   },
   "outputs": [],
   "source": [
    "#Depth Distribution (Histogram & Boxplot)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "fig.suptitle('Earthquake Depth Distribution Analysis (0-700 km)', fontsize=16)\n",
    "\n",
    "axes[0].hist(eq_df['depth'], bins=50, color='darkred', edgecolor='black')\n",
    "axes[0].set_title('Histogram: Frequency of Depth')\n",
    "axes[0].set_xlabel('Depth (km)')\n",
    "axes[0].set_ylabel('Number of Earthquakes (Frequency)')\n",
    "axes[0].set_xlim(0, 700)\n",
    "axes[0].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "\n",
    "axes[1].boxplot(eq_df['depth'], vert=True, patch_artist=True, \n",
    "                boxprops=dict(facecolor='skyblue', color='darkblue'),\n",
    "                medianprops=dict(color='red', linewidth=2))\n",
    "\n",
    "axes[1].set_title('Box Plot: Summary of Depth')\n",
    "axes[1].set_ylabel('Depth (km)')\n",
    "axes[1].set_ylim(0, 700)\n",
    "axes[1].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2bcb8b-56f5-4dd5-b897-6f9f9b5f149f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T15:22:27.760977Z",
     "iopub.status.busy": "2025-12-03T15:22:27.760977Z",
     "iopub.status.idle": "2025-12-03T15:22:28.358680Z",
     "shell.execute_reply": "2025-12-03T15:22:28.357659Z"
    }
   },
   "outputs": [],
   "source": [
    "#Station count\n",
    "\n",
    "nst_data = eq_df['nst'].dropna()\n",
    "magNst_data = eq_df['magNst'].dropna()\n",
    "\n",
    "plt = libs.plt\n",
    "sns = libs.sns\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "fig.suptitle('Distribution Comparison: Station Count (nst vs. magNst)', fontsize=18)\n",
    "\n",
    "max_limit = 100 \n",
    "bins_count = 60\n",
    "\n",
    "sns.histplot(nst_data, bins=bins_count, ax=axes[0], color='teal', edgecolor='black', kde=True)\n",
    "axes[0].set_title('nst: Number of Stations Used (Overall)')\n",
    "axes[0].set_xlabel('Station Count')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_xlim(0, max_limit)\n",
    "axes[0].grid(axis='y', linestyle='--', alpha=0.6)\n",
    "\n",
    "sns.histplot(magNst_data, bins=bins_count, ax=axes[1], color='darkorange', edgecolor='black', kde=True)\n",
    "axes[1].set_title('magNst: Number of Stations Used for Magnitude')\n",
    "axes[1].set_xlabel('Station Count')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_xlim(0, max_limit)\n",
    "axes[1].grid(axis='y', linestyle='--', alpha=0.6)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc5790b-bc0e-4c9b-8090-636d6f9835d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T15:22:28.359692Z",
     "iopub.status.busy": "2025-12-03T15:22:28.359692Z",
     "iopub.status.idle": "2025-12-03T15:22:28.677834Z",
     "shell.execute_reply": "2025-12-03T15:22:28.677834Z"
    }
   },
   "outputs": [],
   "source": [
    "#Distance to the nearest station\n",
    "\n",
    "dmin_data = eq_df['dmin'].dropna()\n",
    "\n",
    "plt = libs.plt\n",
    "sns = libs.sns\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "sns.histplot(dmin_data, bins=50, color='mediumblue', edgecolor='black', kde=True)\n",
    "\n",
    "plt.xlim(0, 5) \n",
    "\n",
    "plt.title('Distribution of Dmin (Distance to Nearest Station)')\n",
    "plt.xlabel('Dmin (Distance in Degrees)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f72d4cf-9283-45e4-b411-f5a5799fd0d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T15:22:28.679859Z",
     "iopub.status.busy": "2025-12-03T15:22:28.679859Z",
     "iopub.status.idle": "2025-12-03T15:22:29.016485Z",
     "shell.execute_reply": "2025-12-03T15:22:29.016485Z"
    }
   },
   "outputs": [],
   "source": [
    "#Temporal Frequency (Monthly / Daily Counts)\n",
    "\n",
    "eq_df['time'] = pd.to_datetime(eq_df['time'])\n",
    "eq_df['day_of_week'] = eq_df['time'].dt.dayofweek\n",
    "eq_df['hour_of_day'] = eq_df['time'].dt.hour\n",
    "\n",
    "plt = libs.plt\n",
    "sns = libs.sns\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "fig.suptitle('Earthquake Frequency by Temporal Period', fontsize=16)\n",
    "\n",
    "day_labels = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "day_counts = eq_df['day_of_week'].value_counts().sort_index()\n",
    "\n",
    "sns.barplot(x=day_counts.index, y=day_counts.values, ax=axes[0], color='lightcoral')\n",
    "axes[0].set_title('Frequency by Day of the Week (GMT/UTC)')\n",
    "axes[0].set_xlabel('Day of the Week')\n",
    "axes[0].set_ylabel('Total Number of Earthquakes')\n",
    "axes[0].set_xticks(range(7))\n",
    "axes[0].set_xticklabels(day_labels)\n",
    "axes[0].grid(axis='y', linestyle='--', alpha=0.6)\n",
    "\n",
    "hour_counts = eq_df['hour_of_day'].value_counts().sort_index()\n",
    "all_hours = pd.Series(0, index=range(24)).add(hour_counts, fill_value=0)\n",
    "\n",
    "sns.lineplot(x=all_hours.index, y=all_hours.values, ax=axes[1], marker='o', color='darkgreen')\n",
    "axes[1].set_title('Frequency by Hour)')\n",
    "axes[1].set_xlabel('Hour')\n",
    "axes[1].set_ylabel('Total Number of Earthquakes')\n",
    "axes[1].set_xticks(range(0, 24, 2))\n",
    "axes[1].grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8f0956",
   "metadata": {},
   "source": [
    "### 8. Multivariate / Spatial / Quality EDA (Part 2)\n",
    "This section interlinks magnitude, depth, spatial location, and the engineered `quality_score` to understand whether severe events align with specific depth ranges or regions and whether instrumentation uncertainty should influence downstream modelling.\n",
    "\n",
    "Each subsection reuses `featured_eq_df` (cleaned + engineered features) and is safe to re-run independently:\n",
    "- **8.1 Magnitude vs Depth** - correlate physical severity with focal depth classes.\n",
    "- **8.2 Global Spatial Distribution** - world epicentre map plus regional share table.\n",
    "- **8.3 Regional Seismicity Comparisons** - compare severity/depth mixes by region.\n",
    "- **8.4 Quality Score & Uncertainty** - surface low-quality measurements before modelling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section8-1-intro",
   "metadata": {},
   "source": [
    "#### 8.1 Magnitude vs Depth\n",
    "Deep-focus earthquakes produce different shaking profiles from shallow crustal events. The scatter + distribution combo below highlights whether high magnitudes cluster at particular depth ranges.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "section8-1-code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T15:22:29.020137Z",
     "iopub.status.busy": "2025-12-03T15:22:29.020137Z",
     "iopub.status.idle": "2025-12-03T15:22:29.864837Z",
     "shell.execute_reply": "2025-12-03T15:22:29.863838Z"
    }
   },
   "outputs": [],
   "source": [
    "sns = libs.sns\n",
    "pd = libs.pd\n",
    "np = libs.np\n",
    "plt = libs.plt\n",
    "\n",
    "depth_mag_df = featured_eq_df[[\"depth\", \"mag\", \"depth_category\", \"mag_category\"]].dropna(subset=[\"depth\", \"mag\"]).copy()\n",
    "if depth_mag_df.empty:\n",
    "    raise ValueError(\"No depth/magnitude samples available for Section 8.1.\")\n",
    "\n",
    "sample_limit = 8000\n",
    "plot_df = depth_mag_df.sample(sample_limit, random_state=42) if len(depth_mag_df) > sample_limit else depth_mag_df\n",
    "corr_value = float(depth_mag_df[\"mag\"].corr(depth_mag_df[\"depth\"]))\n",
    "depth_mag_df[\"depth_bin\"] = pd.cut(\n",
    "    depth_mag_df[\"depth\"],\n",
    "    bins=[0, 50, 100, 200, 400, 700],\n",
    "    include_lowest=True,\n",
    "    duplicates=\"drop\"\n",
    ")\n",
    "\n",
    "depth_order = [cat for cat in [\"shallow\", \"intermediate\", \"deep\"] if cat in depth_mag_df[\"depth_category\"].dropna().unique()]\n",
    "depth_order = depth_order if depth_order else None\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6), gridspec_kw={\"width_ratios\": [2.25, 1]})\n",
    "sns.scatterplot(\n",
    "    data=plot_df,\n",
    "    x=\"depth\",\n",
    "    y=\"mag\",\n",
    "    hue=\"depth_category\",\n",
    "    size=\"mag\",\n",
    "    sizes=(20, 140),\n",
    "    palette=\"viridis\",\n",
    "    alpha=0.65,\n",
    "    ax=axes[0]\n",
    ")\n",
    "axes[0].set_title(\"Magnitude vs Depth (sampled)\")\n",
    "axes[0].set_xlabel(\"Depth (km)\")\n",
    "axes[0].set_ylabel(\"Magnitude (Mw)\")\n",
    "axes[0].grid(True, linestyle=\"--\", alpha=0.35)\n",
    "axes[0].text(\n",
    "    0.02,\n",
    "    0.05,\n",
    "    f\"Pearson r = {corr_value:.2f}\",\n",
    "    transform=axes[0].transAxes,\n",
    "    bbox={\"facecolor\": \"white\", \"alpha\": 0.7, \"edgecolor\": \"gray\"}\n",
    ")\n",
    "\n",
    "sns.boxenplot(\n",
    "    data=depth_mag_df,\n",
    "    x=\"depth_category\",\n",
    "    y=\"mag\",\n",
    "    hue=\"depth_category\",\n",
    "    order=depth_order,\n",
    "    palette=\"viridis\",\n",
    "    dodge=False,\n",
    "    ax=axes[1]\n",
    ")\n",
    "legend = axes[1].get_legend()\n",
    "if legend is not None:\n",
    "    legend.remove()\n",
    "axes[1].set_title(\"Magnitude distribution by depth class\")\n",
    "axes[1].set_xlabel(\"Depth category\")\n",
    "axes[1].set_ylabel(\"Magnitude (Mw)\")\n",
    "axes[1].grid(True, axis=\"y\", linestyle=\"--\", alpha=0.35)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "depth_summary = (\n",
    "    depth_mag_df.dropna(subset=[\"depth_bin\"])\n",
    "    .groupby(\"depth_bin\", observed=False)\n",
    "    .agg(\n",
    "        events=(\"mag\", \"size\"),\n",
    "        median_mag=(\"mag\", \"median\"),\n",
    "        pct_strong=(\"mag\", lambda s: 100 * (s >= 6.0).mean())\n",
    "    )\n",
    ")\n",
    "depth_summary[\"events_pct\"] = depth_summary[\"events\"] / len(depth_mag_df) * 100\n",
    "depth_summary = depth_summary[[\"events\", \"events_pct\", \"median_mag\", \"pct_strong\"]]\n",
    "depth_summary = depth_summary.round({\"events\": 0, \"events_pct\": 1, \"median_mag\": 2, \"pct_strong\": 1})\n",
    "depth_summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section8-1-notes",
   "metadata": {},
   "source": [
    "- **Correlation panel** - Pearson `r` quantifies coupling between depth and magnitude; |r| above ~0.3 suggests physical linkage worth modelling.\n",
    "- **Distribution panel** - boxenplot tails reveal whether deep foci host disproportionately large events.\n",
    "- **Depth-bin table** - cite `events_pct` and `pct_strong` when explaining which depth slices shoulder Mw ≥ 6 activity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section8-2-intro",
   "metadata": {},
   "source": [
    "#### 8.2 Global Spatial Distribution (Epicentre Map)\n",
    "An interactive Plotly globe (with a Matplotlib fallback) shows epicentres scaled by magnitude, while the accompanying table quantifies each macro region's share of global and strong events.\n",
    "\n",
    "- Run the code cell below to render the interactive map inline (or Matplotlib fallback if Plotly is unavailable).\n",
    "- If you need files on disk, toggle `export_epicentre_outputs = True` in the next cell to write HTML/PNG/CSV beside the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "section8-2-code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T15:22:29.867334Z",
     "iopub.status.busy": "2025-12-03T15:22:29.867334Z",
     "iopub.status.idle": "2025-12-03T15:22:31.721916Z",
     "shell.execute_reply": "2025-12-03T15:22:31.721916Z"
    }
   },
   "outputs": [],
   "source": [
    "px = libs.px\n",
    "plt = libs.plt\n",
    "pd = libs.pd\n",
    "np = libs.np\n",
    "\n",
    "map_cols = [\"latitude\", \"longitude\", \"mag\", \"mag_category\", \"broad_region\", \"depth\", \"place\"]\n",
    "available_cols = [c for c in map_cols if c in featured_eq_df.columns]\n",
    "map_df = featured_eq_df[available_cols].dropna(subset=[c for c in [\"latitude\", \"longitude\", \"mag\", \"depth\"] if c in available_cols]).copy()\n",
    "if map_df.empty:\n",
    "    raise ValueError(\"No latitude/longitude samples available for Section 8.2.\")\n",
    "\n",
    "map_df[\"mag_size\"] = np.clip(map_df[\"mag\"], 3, None)\n",
    "map_df[\"is_strong\"] = map_df[\"mag\"] >= 6.0\n",
    "map_df[\"hover_label\"] = (\n",
    "    map_df[\"place\"].fillna(\"Unknown location\")\n",
    "    + \" | Mw \" + map_df[\"mag\"].round(1).astype(str)\n",
    "    + \" | depth \" + map_df[\"depth\"].round(0).astype(int).astype(str) + \" km\"\n",
    ")\n",
    "\n",
    "if px is not None:\n",
    "    fig = px.scatter_geo(\n",
    "        map_df,\n",
    "        lat=\"latitude\",\n",
    "        lon=\"longitude\",\n",
    "        color=\"broad_region\",\n",
    "        size=\"mag_size\",\n",
    "        size_max=18,\n",
    "        hover_name=\"hover_label\",\n",
    "        hover_data={\"mag\":\":.1f\", \"depth\":\":.0f\", \"mag_category\":True, \"broad_region\":True},\n",
    "        projection=\"natural earth\",\n",
    "        template=\"plotly_white\",\n",
    "        title=\"Global epicentre distribution (bubble size ~ magnitude)\"\n",
    "    )\n",
    "    fig.update_layout(legend_title_text=\"Broad region\")\n",
    "    try:\n",
    "        fig.show()\n",
    "    except ValueError as exc:\n",
    "        if \"nbformat\" in str(exc):\n",
    "            import plotly.io as pio\n",
    "            pio.renderers.default = \"browser\"  # fallback when nbformat is missing\n",
    "            fig.show()\n",
    "        else:\n",
    "            raise\n",
    "else:\n",
    "    fig, ax = plt.subplots(figsize=(14, 7))\n",
    "    scatter = ax.scatter(\n",
    "        map_df[\"longitude\"],\n",
    "        map_df[\"latitude\"],\n",
    "        c=map_df[\"mag\"],\n",
    "        cmap=\"plasma\",\n",
    "        s=np.square(map_df[\"mag_size\"]) * 1.5,\n",
    "        alpha=0.6,\n",
    "        linewidths=0.15,\n",
    "        edgecolor=\"black\"\n",
    "    )\n",
    "    ax.set_title(\"Global epicentre distribution (bubble size ~ magnitude)\")\n",
    "    ax.set_xlabel(\"Longitude\")\n",
    "    ax.set_ylabel(\"Latitude\")\n",
    "    ax.set_xlim(-180, 180)\n",
    "    ax.set_ylim(-90, 90)\n",
    "    ax.grid(True, linestyle=\"--\", alpha=0.3)\n",
    "    plt.colorbar(scatter, ax=ax, label=\"Magnitude (Mw)\")\n",
    "\n",
    "region_summary = (\n",
    "    map_df.assign(strong_pct=map_df[\"is_strong\"].astype(float))\n",
    "    .groupby(\"broad_region\")\n",
    "    .agg(\n",
    "        events=(\"mag\", \"size\"),\n",
    "        pct_global=(\"mag\", lambda s: 100 * len(s) / len(map_df)),\n",
    "        pct_strong=(\"strong_pct\", \"mean\"),\n",
    "        median_depth=(\"depth\", \"median\")\n",
    "    )\n",
    "    .sort_values(\"events\", ascending=False)\n",
    ")\n",
    "region_summary[\"pct_strong\"] = region_summary[\"pct_strong\"] * 100\n",
    "region_summary = region_summary.round({\"pct_global\": 1, \"pct_strong\": 1, \"median_depth\": 0})\n",
    "region_summary\n",
    "\n",
    "# Optional: persist Section 8.2 outputs (set True to write files)\n",
    "export_epicentre_outputs = False\n",
    "if export_epicentre_outputs:\n",
    "    OUTPUTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    region_summary.to_csv(OUTPUTS_DIR / \"region_summary_section8_2.csv\")\n",
    "    if px is not None:\n",
    "        fig.write_html(OUTPUTS_DIR / \"epicentre_map_section8_2.html\", include_plotlyjs=\"cdn\")\n",
    "        try:\n",
    "            fig.write_image(OUTPUTS_DIR / \"epicentre_map_section8_2.png\", scale=2)\n",
    "        except ValueError as exc:\n",
    "            if \"kaleido\" in str(exc).lower():\n",
    "                print(\"Plotly static export skipped (kaleido not installed).\")\n",
    "            else:\n",
    "                raise\n",
    "    else:\n",
    "        fig.savefig(OUTPUTS_DIR / \"epicentre_map_section8_2.png\", dpi=300, bbox_inches=\"tight\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e453d44",
   "metadata": {},
   "source": [
    "Run the Section 8.2 cell above to see the map output in-line. Set `export_epicentre_outputs = True` if you also want files written.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section8-2-notes",
   "metadata": {},
   "source": [
    "- **Geospatial bubble map** - hover any bubble to retrieve location, magnitude, and depth. Clusters hugging the Pacific rim visually validate plate-boundary hypotheses.\n",
    "- **Regional share table** - `pct_global` shows contribution to the catalogue, while `pct_strong` highlights where Mw ≥ 6 energy release concentrates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section8-3-intro",
   "metadata": {},
   "source": [
    "#### 8.3 Regional Seismicity Comparisons\n",
    "Quantify how the magnitude and depth mix differs by `broad_region`. The combined bar/line view emphasises volume vs. major-quake share, while the heatmap captures depth proportions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "section8-3-code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T15:22:31.771997Z",
     "iopub.status.busy": "2025-12-03T15:22:31.771997Z",
     "iopub.status.idle": "2025-12-03T15:22:32.212537Z",
     "shell.execute_reply": "2025-12-03T15:22:32.212008Z"
    }
   },
   "outputs": [],
   "source": [
    "sns = libs.sns\n",
    "pd = libs.pd\n",
    "plt = libs.plt\n",
    "\n",
    "region_cols = [\"broad_region\", \"mag\", \"depth\", \"depth_category\"]\n",
    "region_cols = [c for c in region_cols if c in featured_eq_df.columns]\n",
    "region_df = featured_eq_df[region_cols].dropna(subset=[\"broad_region\", \"mag\"]).copy()\n",
    "if region_df.empty:\n",
    "    raise ValueError(\"No regional samples available for Section 8.3.\")\n",
    "\n",
    "region_df[\"is_major\"] = region_df[\"mag\"] >= 7.0\n",
    "\n",
    "region_summary = (\n",
    "    region_df.groupby(\"broad_region\")\n",
    "    .agg(\n",
    "        events=(\"mag\", \"size\"),\n",
    "        median_mag=(\"mag\", \"median\"),\n",
    "        iqr_mag=(\"mag\", lambda s: s.quantile(0.75) - s.quantile(0.25)),\n",
    "        pct_major=(\"is_major\", \"mean\"),\n",
    "        median_depth=(\"depth\", \"median\")\n",
    "    )\n",
    "    .sort_values(\"events\", ascending=False)\n",
    ")\n",
    "region_summary[\"pct_major\"] = region_summary[\"pct_major\"] * 100\n",
    "region_plot = region_summary.reset_index()\n",
    "\n",
    "depth_mix = pd.crosstab(region_df[\"broad_region\"], region_df[\"depth_category\"], normalize=\"index\") * 100\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "sns.barplot(data=region_plot, x=\"broad_region\", y=\"events\", color=\"steelblue\", ax=axes[0])\n",
    "axes[0].set_title(\"Event counts vs. major-quake share\")\n",
    "axes[0].set_xlabel(\"Broad region\")\n",
    "axes[0].set_ylabel(\"Events (count)\")\n",
    "axes[0].tick_params(axis=\"x\", rotation=20)\n",
    "axes[0].grid(True, axis=\"y\", linestyle=\"--\", alpha=0.3)\n",
    "\n",
    "ax2 = axes[0].twinx()\n",
    "sns.lineplot(data=region_plot, x=\"broad_region\", y=\"pct_major\", marker=\"o\", color=\"darkred\", ax=ax2)\n",
    "ax2.set_ylabel(\"% of events Mw >= 7\")\n",
    "ax2.set_ylim(0, max(5, region_plot[\"pct_major\"].max() + 2))\n",
    "ax2.grid(False)\n",
    "\n",
    "if depth_mix.empty:\n",
    "    axes[1].text(0.5, 0.5, \"Depth categories unavailable\", ha=\"center\", va=\"center\", fontsize=12)\n",
    "    axes[1].axis(\"off\")\n",
    "else:\n",
    "    sns.heatmap(\n",
    "        depth_mix,\n",
    "        annot=True,\n",
    "        fmt=\".1f\",\n",
    "        cmap=\"YlOrRd\",\n",
    "        cbar_kws={\"label\": \"% per region\"},\n",
    "        ax=axes[1]\n",
    "    )\n",
    "    axes[1].set_title(\"Depth mix by region (% of row)\")\n",
    "    axes[1].set_xlabel(\"Depth category\")\n",
    "    axes[1].set_ylabel(\"\")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "region_summary = region_summary.round({\"median_mag\": 2, \"iqr_mag\": 2, \"pct_major\": 1, \"median_depth\": 0})\n",
    "region_summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section8-3-notes",
   "metadata": {},
   "source": [
    "- **Volume vs. severity** - the dual-axis figure surfaces whether high-volume regions also shoulder most Mw ≥ 7 events.\n",
    "- **Depth mix heatmap** - row-normalised percentages flag shallow-crust dominated regions vs. deep subduction zones.\n",
    "- **Summary table** - use `iqr_mag` and `median_depth` to articulate variability and tectonic style.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section8-4-intro",
   "metadata": {},
   "source": [
    "#### 8.4 Quality Score & Uncertainty Analysis\n",
    "`quality_score` compresses normalised gap/rms/error metrics into a 0-1 scale (1 = best). This subsection surfaces measurement reliability before modelling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "section8-4-code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T15:22:32.214704Z",
     "iopub.status.busy": "2025-12-03T15:22:32.214704Z",
     "iopub.status.idle": "2025-12-03T15:22:33.569529Z",
     "shell.execute_reply": "2025-12-03T15:22:33.569529Z"
    }
   },
   "outputs": [],
   "source": [
    "sns = libs.sns\n",
    "pd = libs.pd\n",
    "np = libs.np\n",
    "plt = libs.plt\n",
    "\n",
    "quality_base_cols = [\n",
    "    \"quality_score\", \"gap\", \"rms\", \"depthError\", \"magError\", \"horizontalError\",\n",
    "    \"mag\", \"mag_category\", \"broad_region\", \"time\", \"place\", \"id\"\n",
    "]\n",
    "quality_cols = [c for c in quality_base_cols if c in featured_eq_df.columns]\n",
    "quality_df = featured_eq_df[quality_cols].dropna(subset=[\"quality_score\"]).copy()\n",
    "if quality_df.empty:\n",
    "    raise ValueError(\"quality_score not available for Section 8.4.\")\n",
    "\n",
    "quantiles = quality_df[\"quality_score\"].quantile([0.25, 0.5, 0.75]).to_dict()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "sns.histplot(\n",
    "    data=quality_df,\n",
    "    x=\"quality_score\",\n",
    "    bins=30,\n",
    "    color=\"slateblue\",\n",
    "    edgecolor=\"white\",\n",
    "    ax=axes[0]\n",
    ")\n",
    "axes[0].set_title(\"Quality score distribution (1 = best)\")\n",
    "axes[0].set_xlabel(\"quality_score\")\n",
    "axes[0].set_ylabel(\"Event count\")\n",
    "for quant, label, color in [\n",
    "    (0.25, \"25th percentile\", \"#8884d8\"),\n",
    "    (0.50, \"Median\", \"#222222\"),\n",
    "    (0.75, \"75th percentile\", \"#ff7f0e\")\n",
    "]:\n",
    "    axes[0].axvline(quantiles[quant], color=color, linestyle=\"--\", linewidth=1.2, label=f\"{label}: {quantiles[quant]:.2f}\")\n",
    "axes[0].legend(loc=\"best\", frameon=False)\n",
    "axes[0].grid(True, linestyle=\"--\", alpha=0.3)\n",
    "\n",
    "scatter_cols = [c for c in [\"quality_score\", \"magError\", \"depthError\", \"broad_region\"] if c in quality_df.columns]\n",
    "scatter_df = quality_df[scatter_cols].dropna()\n",
    "axes[1].set_title(\"Quality vs. magnitude error\")\n",
    "axes[1].set_xlabel(\"quality_score\")\n",
    "axes[1].set_ylabel(\"magError\")\n",
    "if scatter_df.empty:\n",
    "    axes[1].text(0.5, 0.5, \"Missing error columns\", ha=\"center\", va=\"center\", fontsize=12)\n",
    "    axes[1].grid(False)\n",
    "else:\n",
    "    hue_col = \"broad_region\" if \"broad_region\" in scatter_df.columns else None\n",
    "    size_col = \"depthError\" if \"depthError\" in scatter_df.columns else None\n",
    "    sns.scatterplot(\n",
    "        data=scatter_df,\n",
    "        x=\"quality_score\",\n",
    "        y=\"magError\",\n",
    "        hue=hue_col,\n",
    "        size=size_col,\n",
    "        palette=\"coolwarm\",\n",
    "        alpha=0.7,\n",
    "        ax=axes[1]\n",
    "    )\n",
    "    axes[1].grid(True, linestyle=\"--\", alpha=0.3)\n",
    "    axes[1].invert_xaxis()\n",
    "    axes[1].legend(loc=\"best\")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "agg_dict = {\n",
    "    \"mean_quality\": (\"quality_score\", \"mean\"),\n",
    "    \"p10_quality\": (\"quality_score\", lambda s: s.quantile(0.10)),\n",
    "    \"p90_quality\": (\"quality_score\", lambda s: s.quantile(0.90))\n",
    "}\n",
    "if \"magError\" in quality_df.columns:\n",
    "    agg_dict[\"median_magError\"] = (\"magError\", \"median\")\n",
    "if \"depthError\" in quality_df.columns:\n",
    "    agg_dict[\"median_depthError\"] = (\"depthError\", \"median\")\n",
    "if \"horizontalError\" in quality_df.columns:\n",
    "    agg_dict[\"median_horizontalError\"] = (\"horizontalError\", \"median\")\n",
    "\n",
    "region_quality = quality_df.groupby(\"broad_region\").agg(**agg_dict)\n",
    "region_quality.insert(0, \"events\", quality_df.groupby(\"broad_region\").size())\n",
    "region_quality = region_quality.sort_values(\"mean_quality\", ascending=False)\n",
    "region_quality = region_quality.round(3)\n",
    "\n",
    "detail_cols = [c for c in [\"time\", \"place\", \"mag\", \"quality_score\", \"magError\", \"depthError\", \"horizontalError\", \"gap\", \"rms\"] if c in quality_df.columns]\n",
    "low_quality_events = quality_df.nsmallest(min(5, len(quality_df)), \"quality_score\")[detail_cols]\n",
    "\n",
    "region_quality, low_quality_events\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section8-4-notes",
   "metadata": {},
   "source": [
    "- **Histogram** - cite percentile markers to describe overall measurement fidelity; a tight cluster near 1 indicates robust instrumentation.\n",
    "- **Scatter** - low-quality (left) observations aligning with high `magError` should be down-weighted or excluded during modelling.\n",
    "- **Tables** - region-level metrics and the tail of poorest events (IDs, locations, error terms) guide manual curation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Modelling (strong-event classifier)\n",
    "\n",
    "We turn the engineered catalogue into an early-warning classifier for strong earthquakes (Mw >= 6.0) and probe what drives those predictions. The workflow below is intentionally opinionated: class-weighted logistic regression as a calibrated baseline, plus a non-linear forest to capture interactions, all wrapped in a single preprocessing pipeline to prevent leakage.\n",
    "\n",
    "Key design choices:\n",
    "- Temporal/geospatial leakage: stratified split by the strong-label, with an option to filter low-quality measurements; keep the feature pipeline coupled to the model.\n",
    "- Features: physical (depth, latitude/longitude, uncertainty terms, quality_score), categorical context (hemisphere, broad_region, season/part_of_day), and engineered distances from the equator/prime meridian.\n",
    "- Metrics: F1/PR-AUC for the positive class, ROC-AUC for calibration sanity, and sliced scores by broad_region to check generalisation.\n",
    "- Visuals: confusion matrix, ROC/PR curves, and top feature importances (forest) for interpretability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.1 Data split, pipeline, and models\n",
    "We assemble a modelling dataframe from `featured_eq_df`, define numeric/categorical feature groups, and train two models: a class-weighted logistic regression baseline and a compact random forest. Both share the same preprocessing (median-impute + scale numeric; one-hot encode categoricals).\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    precision_recall_curve, roc_curve, auc, average_precision_score,\n",
    "    confusion_matrix, classification_report\n",
    ")\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "if not libs.HAS_SKLEARN or libs.plt is None:\n",
    "    print(\"scikit-learn/matplotlib missing; install to run Section 9.\")\n",
    "else:\n",
    "    model_df = featured_eq_df.copy()\n",
    "    model_df = model_df.dropna(subset=[\"mag\", \"depth\", \"latitude\", \"longitude\"]).copy()\n",
    "    model_df[\"strong_quake\"] = model_df.get(\"is_strong_quake\", model_df[\"mag\"] >= 6.0).astype(int)\n",
    "\n",
    "    if model_df[\"strong_quake\"].nunique() < 2:\n",
    "        raise ValueError(\"Need both strong and non-strong examples for modelling.\")\n",
    "\n",
    "    numeric_features = [c for c in [\n",
    "        \"depth\", \"latitude\", \"longitude\", \"gap\", \"rms\", \"depthError\", \n",
    "        \"magError\", \"horizontalError\", \"quality_score\", \"distance_from_equator_km\", \n",
    "        \"distance_from_prime_meridian_km\"\n",
    "    ] if c in model_df.columns]\n",
    "    categorical_features = [c for c in [\n",
    "        \"broad_region\", \"hemisphere_NS\", \"hemisphere_EW\", \"part_of_day\", \"season\", \"depth_category\", \"mag_category\"\n",
    "    ] if c in model_df.columns]\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", Pipeline([\n",
    "                (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "                (\"scaler\", StandardScaler()),\n",
    "            ]), numeric_features),\n",
    "            (\"cat\", Pipeline([\n",
    "                (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "                (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "            ]), categorical_features),\n",
    "        ],\n",
    "        remainder=\"drop\",\n",
    "    )\n",
    "\n",
    "    X = model_df[numeric_features + categorical_features]\n",
    "    y = model_df[\"strong_quake\"]\n",
    "    stratify = y if y.nunique() > 1 else None\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=stratify\n",
    "    )\n",
    "\n",
    "    log_reg = Pipeline(steps=[(\"preprocess\", preprocessor), (\"clf\", LogisticRegression(max_iter=1000, class_weight=\"balanced\"))])\n",
    "    forest = Pipeline(steps=[(\"preprocess\", preprocessor), (\"clf\", RandomForestClassifier(\n",
    "        n_estimators=200, max_depth=None, min_samples_leaf=2, class_weight=\"balanced_subsample\", random_state=42\n",
    "    ))])\n",
    "\n",
    "    models = {\"LogReg\": log_reg, \"Forest\": forest}\n",
    "    eval_rows = []\n",
    "\n",
    "    for name, model in models.items():\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_prob = model.predict_proba(X_test)[:, 1]\n",
    "        precision, recall, _ = precision_recall_curve(y_test, y_prob)\n",
    "        pr_auc_curve = auc(recall, precision)\n",
    "        avg_precision = average_precision_score(y_test, y_prob)\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        report = classification_report(y_test, y_pred, output_dict=True)\n",
    "\n",
    "        eval_rows.append({\n",
    "            \"model\": name,\n",
    "            \"accuracy\": report[\"accuracy\"],\n",
    "            \"precision_pos\": report[\"1\"][\"precision\"],\n",
    "            \"recall_pos\": report[\"1\"][\"recall\"],\n",
    "            \"f1_pos\": report[\"1\"][\"f1-score\"],\n",
    "            \"roc_auc\": roc_auc,\n",
    "            \"pr_auc\": pr_auc_curve,\n",
    "            \"avg_precision\": avg_precision,\n",
    "        })\n",
    "\n",
    "    metrics_df = pd.DataFrame(eval_rows).set_index(\"model\").round(3)\n",
    "    display(metrics_df)\n",
    "\n",
    "    # Save artifacts for plots below\n",
    "    best_model = log_reg\n",
    "    best_model.fit(X_train, y_train)\n",
    "    y_pred_best = best_model.predict(X_test)\n",
    "    y_prob_best = best_model.predict_proba(X_test)[:, 1]\n",
    "    pr_curve = precision_recall_curve(y_test, y_prob_best)\n",
    "    roc_curve_vals = roc_curve(y_test, y_prob_best)\n",
    "    conf_mat = confusion_matrix(y_test, y_pred_best)\n",
    "    test_context = X_test.assign(broad_region=model_df.loc[X_test.index, \"broad_region\"] if \"broad_region\" in model_df.columns else \"global\")\n",
    "    region_slices = []\n",
    "    for region, idxs in test_context.groupby(\"broad_region\").groups.items():\n",
    "        y_true_region = y_test.loc[idxs]\n",
    "        y_pred_region = y_pred_best[idxs]\n",
    "        if y_true_region.nunique() < 2:\n",
    "            continue\n",
    "        region_slices.append({\n",
    "            \"region\": region,\n",
    "            \"support\": len(y_true_region),\n",
    "            \"recall_pos\": classification_report(y_true_region, y_pred_region, output_dict=True)[\"1\"][\"recall\"],\n",
    "            \"f1_pos\": classification_report(y_true_region, y_pred_region, output_dict=True)[\"1\"][\"f1-score\"],\n",
    "        })\n",
    "    region_report = pd.DataFrame(region_slices).sort_values(\"f1_pos\", ascending=False) if region_slices else pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.2 Evaluation visuals (confusion, ROC, PR)\n",
    "Confusion matrix highlights false alarms vs. misses; ROC/PR show threshold trade-offs for the strong class. The per-region slice table checks generalisation across tectonic settings.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if not libs.HAS_SKLEARN or libs.plt is None:\n",
    "    print(\"scikit-learn/matplotlib missing; install to run Section 9 plots.\")\n",
    "elif 'conf_mat' not in globals():\n",
    "    print(\"Run the Section 9.1 cell first.\")\n",
    "else:\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "    sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues', cbar=False, ax=axes[0])\n",
    "    axes[0].set_title('Confusion matrix (LogReg)')\n",
    "    axes[0].set_xlabel('Predicted'); axes[0].set_ylabel('Actual')\n",
    "    axes[0].set_xticklabels(['Not strong', 'Strong']); axes[0].set_yticklabels(['Not strong', 'Strong'])\n",
    "\n",
    "    precision, recall, _ = pr_curve\n",
    "    fpr, tpr, _ = roc_curve_vals\n",
    "    axes[1].plot(fpr, tpr, label=f'ROC AUC = {auc(fpr, tpr):.3f}', color='darkorange')\n",
    "    axes[1].plot([0,1],[0,1],'--', color='gray', linewidth=1)\n",
    "    axes[1].set_title('ROC curve'); axes[1].set_xlabel('False positive rate'); axes[1].set_ylabel('True positive rate')\n",
    "    axes[1].legend(loc='lower right')\n",
    "\n",
    "    axes[2].plot(recall, precision, color='seagreen', label=f'PR AUC = {auc(recall, precision):.3f}')\n",
    "    axes[2].set_title('Precision-Recall curve'); axes[2].set_xlabel('Recall'); axes[2].set_ylabel('Precision')\n",
    "    axes[2].legend(loc='lower left')\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if not region_report.empty:\n",
    "        display(region_report.round(3))\n",
    "    else:\n",
    "        print('Region-level slice unavailable (insufficient variability).')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.3 Drivers of strong-quake predictions\n",
    "Random forest importances (post one-hot encoding) surface the most influential variables; depth/latitude and quality/uncertainty terms should dominate if signal-rich.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if not libs.HAS_SKLEARN or libs.plt is None:\n",
    "    print(\"scikit-learn/matplotlib missing; install to run feature importance.\")\n",
    "elif 'forest' not in globals():\n",
    "    print(\"Run the Section 9.1 cell first.\")\n",
    "else:\n",
    "    # Refit forest to access fitted encoder feature names\n",
    "    forest.fit(X_train, y_train)\n",
    "    # Get feature names after preprocessing\n",
    "    cat_encoder = forest.named_steps['preprocess'].named_transformers_['cat'].named_steps['encoder']\n",
    "    cat_feature_names = list(cat_encoder.get_feature_names_out(categorical_features)) if categorical_features else []\n",
    "    feature_names = numeric_features + cat_feature_names\n",
    "    importances = forest.named_steps['clf'].feature_importances_\n",
    "    top_n = 12\n",
    "    top_idx = importances.argsort()[::-1][:top_n]\n",
    "    top_features = [(feature_names[i], importances[i]) for i in top_idx]\n",
    "    feat_df = pd.DataFrame(top_features, columns=['feature', 'importance'])\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(data=feat_df, x='importance', y='feature', palette='viridis')\n",
    "    plt.title('Top feature importances (RandomForest)')\n",
    "    plt.xlabel('Gini importance'); plt.ylabel('Feature')\n",
    "    plt.tight_layout()\n",
    "    display(feat_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.4 Interpretation\n",
    "- **Performance**: prioritise PR-AUC and F1 for the positive class; ROC can overstate performance on imbalanced data. Tune the decision threshold based on operational tolerance for false alarms.\n",
    "- **Slices**: regional recall/F1 quickly reveal whether the model overfits to high-density subduction zones; consider per-region calibration or hierarchical models if gaps persist.\n",
    "- **Drivers**: if importances elevate uncertainty fields (gap/rms/errors), consider capping their influence via monotonic constraints or reweighting by `quality_score`. Depth and latitude should remain dominant physical predictors.\n",
    "- **Next steps**: perform temporal cross-validation, add proximity-to-plate-boundary features, and calibrate probabilities (e.g., isotonic) before deployment in an alerting pipeline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40cce0d",
   "metadata": {},
   "source": [
    "### 10. Discussion & Conclusion\n",
    "\n",
    "#### 10.1 Key Findings\n",
    "- Global epicentres cluster along subduction belts (notably the Pacific rim), with shallow crustal events dominating counts while intermediate/deep slices still host notable strong events.\n",
    "- Magnitude vs depth shows modest correlation; high-magnitude events are not confined to the deepest bins, reinforcing the need to model across depth classes.\n",
    "- Quality-score analysis indicates most events are measured with high fidelity, but a thin low-quality tail (high `magError`/`gap`) should be down-weighted or reviewed before modelling.\n",
    "\n",
    "#### 10.2 Strengths of the Analytical Pipeline\n",
    "- Reusable cleaning and feature-engineering functions shared between the notebook and export scripts keep results reproducible.\n",
    "- Interactive spatial visualisations paired with summary tables make regional patterns and uncertainty interpretable for non-technical readers.\n",
    "- Uncertainty-aware metrics (quality-score slicing, error columns) are surfaced before any predictive step.\n",
    "\n",
    "#### 10.3 Limitations\n",
    "- Catalogue completeness varies by region/time; smaller events are under-reported, biasing magnitude and depth distributions.\n",
    "- `broad_region` is longitude-based and coarse; no focal mechanisms or tectonic setting variables are included yet.\n",
    "- Modelling remains a blueprint in this notebook; performance claims await an implemented training run.\n",
    "\n",
    "#### 10.4 Future Work\n",
    "- Train and validate the outlined classifier/regressor with temporal cross-validation and calibrated probabilities.\n",
    "- Enrich geography with plate-boundary shapefiles and proximity features; add geodetic/elevation context where available.\n",
    "- Automate monthly refreshes (data pull, cleaning, plots, model scoring) to keep the dashboard current.\n",
    "\n",
    "#### 10.5 Conclusion\n",
    "The EDA establishes data health, spatial clustering, and uncertainty structure. Next steps are to deploy an uncertainty-aware classifier, monitor it over time, and keep incorporating new catalogue data to refine both the map and the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190869fb-3270-44bc-a74c-806b2383fa9d",
   "metadata": {},
   "source": [
    "### References\n",
    "- USGS Earthquake Catalog (https://earthquake.usgs.gov/earthquakes/search/) for event data.\n",
    "- Python stack: pandas, numpy, seaborn, matplotlib, plotly (interactive maps), scikit-learn (modelling pipeline suggestions).\n",
    "- Basemaps and tiles: Natural Earth (geo projection) and OpenStreetMap tiles consumed through Plotly mapbox/maplibre.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "earthquake",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}